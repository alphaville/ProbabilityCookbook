\documentclass[a4paper,10pt]{article}
\usepackage{hyperref}

\usepackage{mathematix}
\usepackage[margin=1in,right=1in]{geometry}
\usepackage{lipsum}
\usepackage{enumerate}

\usepackage{natbib}
\usepackage{bibentry}
\nobibliography*

\renewcommand{\cov}{\operatorname{cov}}




\hypersetup{
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={Probability Theory Cookbook},    % title
    pdfauthor={Pantelis Sopasakis},     % author
    pdfsubject={},   % subject of the document
    pdfcreator={P. Sopasakis},   % creator of the document
    pdfproducer={P. Sopasakis}, % producer of the document
    pdfnewwindow=true,      % links in new PDF window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\newcommand{\ce}[1]{\E\left[#1 {}\mid{} \HH \right]}

% Update \bibentry so that bibentries appear in blue
\let\oldbibentry\bibentry
\renewcommand{\bibentry}[1]{{\color{blue} \oldbibentry{#1}}}

\title{Probability Cookbook}
\author{Pantelis Sopasakis}
\begin{document}
\maketitle
\tableofcontents

\begin{abstract}
 This document is intended to serve as the white pages of general probability
 theory and it can be used for a quick brush up or as a quick reference or 
 cheat sheet, but not as primary tutorial material.
\end{abstract}

\section{General Probability Theory}

\subsection{Measurable and Probability spaces}

\begin{enumerate}
 \item ($\sigma$-algebra). Let $X$ be a nonempty set. A collection $\F$ of subsets of $X$
       is called a $\sigma$-algebra if (i) $X \in \F$, (i) $A^c\in\F$ whenever $A\in\F$,
       (ii) if $A_1,\ldots, A_n\in\F$, then $\bigcup_{i=1,\ldots,n}A_i\in\F$. The space $X$
       equipped with a $\sigma$-algebra $\F$ is called a \textit{measurable space}.
       
 \item (d-system) A collection $\mathcal{D}$ of subsets of $X$ is called a d-system or a Dynkin class if 
       (i) $X\in\mathcal{D}$,
       (ii) $A\setminus B\in\mathcal{D}$ whenever $A,B\in\mathcal{D}$ and $A\supseteq B$,
       (iii) $A\in\mathcal{D}$ whenever $A_n\in \mathcal{D}$ and $A_n \uparrow A$ (meaning, 
       $A_{k}\subseteq A_{k+1}$ and $\bigcup_{k\in\N}A_k=A$).
       
 \item (p-system). A collection of sets $\mathcal{P}$ in $X$ is called a p-system
       if $A\cap B\in \mathcal{P}$ whenever $A,B\in\mathcal{P}$.
       
 \item A collection of sets is a $\sigma$-algebra if and only if it is both a p- and a d-system.
 
 \item (Smallest $\sigma$-algebra). Let $\HH$ be a collection of sets in $X$. The smallest collection of sets
       which contains $\HH$ and is a $\sigma$-algebra exists and is denoted by $\sigma(\HH)$.
 
 \item (Monotone class theorem). If a d-system $\mathcal{D}$ contains a p-system $\mathcal{P}$, then is also contains $\sigma(\mathcal{P})$.
 
 \item \label{mps1311949}
       (Borel $\sigma$-algebra). On $\Re$, the $\sigma$-algebra $\sigma(\{(a,b); a<b\})$ is called the Borel $\sigma$-algebra on $\Re$
       which we denote by $\B_\Re$. For topological spaces $(X,\tau)$, the Borel $\sigma$-algebra is
       defined as $\B_X = \sigma(\tau)$, i.e., it is the smallest $\sigma$-algebra which contains
       all open sets. $\B_\Re$ is generated by:
       \begin{enumerate}[i.]
        \item The open intervals $(a,b)$
        \item The closed intervals $[a,b]$
        \item All sets of the form $[a,b)$ or $(a,b]$
        \item Open rays $(a,\infty)$ or $(-\infty,a)$
        \item Closed rays $[a,\infty)$ or $(-\infty,a]$
       \end{enumerate}            

 \item (Measure). A function $\mu: \F\to [0,+\infty]$ is called a measure if 
       for every sequence of disjoint sets $A_n$ from $\F$, $\mu(\bigcup_n A_n)= \sum_n \mu(A_n)$.

 \item \label{mps1311960}
      (Properties of measures). The following hold:
      \begin{enumerate}[i.]
       \item (Empty set is negligible). $\mu(\varnothing)=0$ [Indeed, $\mu(A) = \mu(A\cup \varnothing) = \mu(A) + \mu(\varnothing)$ for all $A\in\F$]
       \item (Monotonicity). $A\subseteq B$ imples $\mu(A) \leq \mu(B)$ [Indeed, $\mu(B) = \mu(A\cup (B\setminus A))$]
       \item (Boole's inequality). For all $A_n\in\F$, $\mu(\bigcup_n A_n) \leq \sum_n \mu(A_n)$
       \item (Sequential continuity). If $A_n\uparrow A$, then $\mu(A_n)\uparrow \mu(A)$.
      \end{enumerate}

 \item (Equality of measures). Let $\mu,\nu$ be two measures on a measurable space $(X,\F)$ and let $\G$ 
       be a p-system generating $\F$. If $\mu(A) = \nu(A)$ for all $A\in \G$, then $\mu(B) = \nu(B)$
       for all $B\in\F$. As presented in \#\ref{mps1311949} above, p-systems are often available and 
       have simple forms.
      
 \item (Completeness). A measure space $(X,\F,\mu)$ is called \textit{complete} if the following holds:
	\[
	    A \in \F, \mu(A)=0, B\subseteq A \Rightarrow B \in \F.
	\]
       Of course, by the monotonicity property in \#\ref{mps1311960}--iii, if $(X,\F,\mu)$ is a complete 
       measure space then $\mu(B) = 0$.
       
 \item (Completion). Let $(X,\F,\mu)$ be a measure space and define the set of \textit{negligible sets} of $\mu$ as 
       $Z_\mu = \{N \subseteq X: \exists N'\supseteq N, N'\in\F \text{ s.t. } \mu(N')=0\}$.
       Let $\F'$ be the $\sigma$-algebra generated by $\F\cup Z_\mu$. Then
       \begin{enumerate}[i.]
        \item Every $B\in\F'$ can be written as $B=A\cup N$ with $A\in\F$ and $N\in Z_\mu$
        \item Define $\mu'(A\cup N) = \mu(A)$; this is a measure on $(X,\F')$ which renders 
              the space $(X,\F',\mu')$ complete. 
       \end{enumerate}
 
 \item (Lebesgue measure on $\Re$ and $\Re^n$). It suffices to define the \textit{Lebesgue measure} on $(\Re,\B_\Re)$
       on the p-system $\{(a,b), a<b\}$; it is $\lambda((a,b))=b - a$. This extends to a measure on  $(\Re,\B_\Re)$.
       Likewise, the collection of $n$-dimensional rectangles $\{(a_1, b_1)\times\ldots \times (a_n, b_n)\}$ is a p-system
       which generates $\B_{\Re^n}$; the Lebesgue measure on $(\Re^n, \B_{\Re^n})$ is 
       $\lambda(\prod_{i=1}^n (a_i, b_i))=\prod_{i=1}^n (b_i-a_i)$.
 
 \item (Lebesgue measurable sets). The completion of the Lebesgue measure defines the class of Lebesgue-measurable
       sets. 
             
 \item (Negligible boundary). If a set $C\subseteq \Re^n$ has a boundary whose Lebesgue measure is $0$, then 
       $C$ is Lebesgue measurable.
\end{enumerate}

\subsection{Random variables}\label{sec:random_variables}
\begin{enumerate}

 \item (Measurable function). A function $f:(X,\F)\to (Y,\G)$ (between two measurable spaces) is 
       called \textit{measurable} if $f^{-1}(G) \in \F$ for all $G\in\G$ (i.e., if it inverts all 
       measurable sets to measurables ones).
       
 \item (Measurability test). Let $\F,\G$ be $\sigma$-algebras on the nonempty sets $X$ and $Y$. Let $\G'$ be 
       a p-system which generates $\G$. A function $f: (X,\F)\to (Y,\G)$ is measurable 
       if and only if $f^{-1}(G')\in \F$ for all $G'\in\G'$ (it suffices to check the 
       measurability condition on a p-system).
       
 \item ($\sigma$-algebra generated by $f$). Let $f:(X,\F)\to (Y,\G)$ (between two measurable spaces) be a measurable
       function. The set 
       \[
        \sigma(f) \dfn \{f^{-1}(B){}\mid{} B\in \G\},
       \]
       is a sub-$\sigma$-algebra of $\F$ and is called the $\sigma$-algebra generated by $f$.
 
 \item (Sub/sup-level sets) Let $f:(X,\F)\to \Re$. The following are equivalent:
      \begin{enumerate}[i.]
       \item $f$ is measurable,
       \item Its \textit{sublevel sets}, that is
       sets of the form $\lev_{\leq \alpha} f \dfn \{x\in X: f(x) \leq \alpha\}$ are measurable,
	\item Its \textit{suplevel sets},
       that is sets of the form $\lev_{ \geq \alpha} f \dfn \{x\in X: f(x) \geq \alpha \}$ are 
       measurable. 
      \end{enumerate}

       
 \item \label{rv220000}
       (Random variable).
       A real-valued random variable $X:\ofp \to (\Re, \B_\Re)$ is a measurable function $X$
       from a probability space $\ofp$ to $\Re$, equipped with the Borel $\sigma$-algebra, that is, 
       for every Borel set $B$, $X^{-1}(B)\in\F$.              
       
 \item \label{rv221030}
      Every nonnegative (real-valued) random variable $X$ on $(\Re_+, \B_{{\Re}_+})$ 
      is written as 
      \[
        X(\omega) = \int_0^{+\infty} 1_{X(\omega)\geq t}\,\d t.
      \]

 \item (Increasing functions).  Every increasing function $f:\Re\to\barre$ is Borel-measurable.
 
 \item (Semicontinuous functions). 
 \item (Pushforward measure)~[\ref{cite:cinlar2011}]. Given measurable spaces $(\mathcal{X},\F)$ and $(\mathcal{Y}, \mathcal{G})$, 
 a measurable mapping $f: X \to Y$ and a (probability) measure $\mu$ on $(\mathcal{X},\F)$, the \textit{pushforward} of $\mu$
 is defined to be a measure $f∗(\mu)$ on $(\mathcal{Y}, \mathcal{G})$ given by
 \[
  (f_*\mu)(B) = \mu(f^{-1}(B)) = \mu(\{\omega\mid f(\omega)\in B\}),
 \]
 for $B\in\mathcal{G}$.
 \item (Change of variables). Let $F$ be a random variable on the probability space $\ofp$ and $F_*\prob$ 
 is the pushforward measure. random variable $X$ is integrable with respect to the pushforward measure $F_*\prob$
 if and only if $X\circ F$ is $\prob$-integrable. Then, the integrals coincide
 \[
  \int X \d(F_*\prob) = \int (X\circ F) \d \prob.
 \]
 \item (Measures from random variables). Let $X$ be a random variable on $\ofp$. 
       We may use $X$ to define the following measure
       \[
        \nu(A) = \int_A X\d \prob,
       \]
       defined for $A\in\F$. This is a positive measure which for short we denote as $\nu=X\prob$
       and it satisfies:
       \[
        \int_A Y\d \nu = \int_A XY\d \prob,
       \]
       for all random variables $Y$.
       
 \item (Compositions). Let $f:(X,\F_X)\to (Y, \F_Y)$ and $g:(Y,\F_Y)\to (Z,\F_Z)$ be two measurable functions. 
       Then, the function $h:(X,\F_X)\ni x\mapsto h(x) \dfn f(g(x)) \in (Z,\F_Z)$ is measurable. 
       
 \item (Characterization of measurability). A function $f:(X,\F)\to\Re$ is $\F$-measurable if and only if 
       it is the pointwise limit of a sequence of simple functions. A function $f:(X,\F)\to\Re_+$ is 
       $\F$-measurable if and only if  it is the pointwise limit of an increasing sequence of simple functions. 
       
 \item (Continuity and measurability). Every continuous function $f:(X,\F)\to\barre$ is Borel-measurable. 
  
 \item (Monotone class of functions). Let $M$ be a collection of functions $f:(X,\F)\to\barre$; let $M_+$
       be all positive functions in $M$ and $M_b$ all bounded functions in $M$. We say that $M$ is a \textit{monotone class}
       of functions if (i) $1\in M$, (ii) if $f,g\in M_b$ and $a,b\in \Re$, then $af+bg\in M$ and (iii)
       if $(f_n)_n\subseteq M_+$ and $f_n \uparrow f$, then $f\in M$.
       
 \item (Monotone class theorem for functions). Let $M$ be a monotone class of functions on $(X,\F)$. Suppose 
       that $\F$ is generated by some p-system $\mathcal{C}$, $1_A \in M$ for all $A\in\mathcal{C}$.
       Then, $M$ includes all positive $\F$-measurable functions and all bounded $\F$-measurable functions. 
               
\end{enumerate}

\subsection{Limits}
\begin{enumerate}
 \item (Lebesgue's monotone convergence theorem). Let $(f_n)_n$ be an increasing sequence of 
       nonnegative Borel functions and let $f \dfn \lim_n f_n$ (in the sense $f_n\to f$ pointwise a.e.). 
       Then $\E[f_n] \uparrow \E[f]$.
      
 \item 	(Lebesgue's Dominated Convergence Theorem). Let $X_n$ be real-valued RVs over $\ofp$. 
	Suppose that $X_n$ converges pointwise to $X$ and is \textit{dominated} by a 
	$Y\in\mathcal{L}_1\ofp$, that is $|X_n|\leq Y$ $\prob$-a.s for all $n\in\N$. 
	Then, $X\in\mathcal{L}_1\ofp$
	and
	\[
	 \lim_n \E[|X_n-X|] = 0,
	\]
        which implies
        \[
         \lim_n \E[X_n] = \E[X].
        \]

 \item (Dominated convergence in $\mathcal{L}^p$).
       For $p\in[1,\infty)$ and a sequence of random variables $X_k:\ofp\to\barre$,
       assume that $X_k\to X$ almost everywhere ($X(\omega)=\lim_k X_k(\omega)$ $\prob$-a.e.)
       and there is $Y\in\mathcal{L}^p\ofp$ so that $X_k\leq Y$. Then, 
       \begin{enumerate}[i.]
        \item $X_k\in\mathcal{L}^p\ofp$ for all $k\in\N$,
        \item $X\in\mathcal{L}^p\ofp$ 
        \item $X_k\to X$ in $\mathcal{L}^p\ofp$, that is $\lim_k \|X_k-X\|_p = 0$.
       \end{enumerate}

 \item (Consequence of the dominated convergence theorem)~[\ref{cite:DWalnut2011}]. 
       Let $\{E_k\}_{k=1}^{\infty}$ be a collection of disjoint events and let $E=\bigcup_{k}E_k$.
       Then,
       \[
        \int_E f = \sum_{k=1}^{\infty} \int_{E_k} f.
       \]

 \item  (Bounded convergence). If $X_k \to X$ almost surely and $\sup_k |X_k| \leq b$
        for some constant $b>0$, then $\E[X_k]\to \E[X]$ and $\E[|X|] \leq b$.
       
 \item 	(Fatou's lemma). Let $X_n\geq 0$ be a sequence of random variables. 
	Then, 
	\[
	\E[\liminf_n X_n] \leq  \liminf_n \E[X_n].
	\]
 \item 	(Fatou's lemma with varying measures). For a sequence of nonnegative random variables $X_n\geq 0$ over $\ofp$,
	and a sequence of (probability) measures $\mu_n$ which converge strongly to a (probability)
	measure $\mu$ (that is, $\mu_n(A)\to\mu(A)$ for all $A\in\F$), we have
	\[
	 \E_\mu[\liminf_n X_n]\leq \liminf_n \E_{\mu_n}[ X_n]
	\]

 \item 	(Reverse Fatou's lemma). Let $X_n\geq 0$ be a sequence of nonnegative random variables over $\ofp$ and
	assume there is a $Y\in\mathcal{L}_1\ofp$ so that $X_n\leq Y$. Then
	\[ 
	 \limsup_n \E[X_n] \leq \E[\limsup_n X_n]
	\]
 \item (Integrable lower bound). 	
	Let $X_n$ be a sequence of random variables over $\ofp$. Suppose, there exists a
	$Y\geq 0$ such that $X_n\geq -Y$ for all $n\in\N$. Then,
	\[
	\E[\liminf_n X_n] \leq  \liminf_n \E[X_n].
	\]
 \item (Beppo Levi's Theorem).
	Let $X_k$ be a sequence of nonnegative random variables on $\ofp$ with $0 \leq X_1 \leq X_{2} \leq \ldots$. 
	Let $X(\omega) = \lim_{k\to\infty}X_k(\omega)$. Then $X$ is a random variable and 
	\[
	 \lim_{k\to\infty} \E[X_k] = \E[\lim_{k\to\infty} X_k].
	\]
 \item (Beppo Levi's Theorem for series).
       Let $X_k$ be a sequence of nonnegative integrable random variables on $\ofp$
       and let $Y_k = \sum_{j=0}^k X_k$. Assume that $\sum_{k=1}^{\infty} \E[Y_k]$ converges.
       Then $Y_k$ satisfies the conditions of the BL theorem and
       \[
        \sum_{k=1}^{\infty} \E[Y_k] = \E \left[\sum_{k=1}^{\infty}Y_k\right].
       \]

 \item (Uniform integrability -- definition)~[\ref{cite:KSigman2009}]. A collection $\{X_k\}_{k\in T}$ is said to be \textit{uniformly
        integrable} if $\sup_{t\in T}\E[|X_t| 1_{|X_t|>x}] \to 0$ as $x\to\infty$.
        
 \item (Constant absolutely integrable sequences as uniformly integrable)~[\ref{cite:KSigman2009}]. The sequence $\{Y\}_{t\in T}$
       with $\E[|Y|]<\infty$ is uniformly integrable.

 \item (Uniform boundedness in $\mathcal{L}_p$, $p>1$, implies uniform integrability).
       If $\{X_t\}_{t\in T}$ is uniformly bounded in $\mathcal{L}_p$, $p>1$ (that is, 
       $\E[|X_k|^p] < c$ for some $c>0$), then it is uniformly integrable.
       
 \item (Convergence under uniform integrability)~[\ref{cite:KSigman2009}]. If $X_k \to X$ a.s. and $\{X_k\}_k$ is uniformly 
       integrable then
       \begin{enumerate}[i.]
         \item $\E[X]< \infty$
         \item $\E[X_k] \to \E[X]$        
        \item $\E|X_k-X|\to 0$
       \end{enumerate}

\end{enumerate}


\subsection{The Radon-Nikodym Theorem}
\begin{enumerate}
 \item (Absolute continuity).
       Let $(\mathcal{X}, \mathscr{G})$ be a measurable space and $\mu$ and $\nu$ two measures on it.
       We say that $\nu$ is \textit{absolutely continuous} with respect to $\mu$ if
       for all $A\in\mathscr{G}$, $\nu(A)=0$ whenever $\mu(A)=0$. We denote this by $\nu\ll\mu$.
 \item (Radon-Nikodym). Let $(\mathcal{X}, \mathscr{G})$ be a measurable space, let $\nu$ be a \textit{$\sigma$-finite}
       measure on $(\mathcal{X}, \mathscr{G})$ which is {absolutely continuous} with respect 
       to a measure $\mu$ on $(\mathcal{X}, \mathscr{G})$. Then, there is a measurable function $f:\mathcal{X}\to[0,\infty)$
       such that for all $A\in \mathcal{G}$
       \[
        \nu(A) = \int_A f \d \mu.
       \]
      This function is denoted by $f=\frac{\d\nu}{\d \mu}$.
 \item (Linearity). Let $\nu$, $\mu$ and $\lambda$ be $\sigma$-finite measures on $(\mathcal{X}, \mathscr{G})$ and $\nu\ll\lambda$, $\mu\ll\lambda$.
       Then
       \[
        \frac{\d(\nu+\mu)}{\d \lambda} = \frac{\nu}{\d \lambda} + \frac{\nu}{\d \lambda},\ \lambda\text{-a.e.}
       \]
 \item (Chain rule). If $\nu\ll\mu\ll\lambda$,
 \[
  \frac{\d\nu}{\d\lambda} = \frac{\d\nu}{\d\mu} \frac{\d\mu}{\d\lambda},\ \lambda\text{-a.e.} 
 \]
 \item (Inverse). If $\nu\ll\mu$ and $\mu\ll\nu$, then
 \[
  \frac{\d \mu}{\d \nu} = \left( \frac{\d \nu}{\d \mu}\right)^{-1},\ \nu\text{-a.e.}
 \]
 \item (Change of measure).
 If $\mu\ll\lambda$ and $g$ is a $\mu$-integrable function, then
 \[
  \int_{\mathcal{X}} g \d \mu = \int_{\mathcal{X}} g \frac{\d \mu}{\d \lambda}\d \lambda.
 \]
 \item (Change of variables in integration). This was addressed using the pushforward. 
 \[
  \E[g(X)] = \int g\circ X\d\prob = \int_\Re g \d(X_*\prob).
 \]
 If the measure $\d(X_*\prob)$ is absolutely continuous with respect to the Lebesgue 
 measure $\mu$ (on $(\Re, \B_\Re)$, then, the Radon-Nikodym derivative $f_X\dfn \frac{\d(X_*\prob)}{\d \mu}$,
 where $f_X:\Re\to\Re$ exists. Then
 \[
  \E[g(X)] = \int_\Re g \d(X_*\prob) = \int_\Re g f_X \d\mu = \int_\Re g(\tau)f_X(\tau)\d \tau.
 \]
 This is known as the \textit{law of the unconscious statistician} (LotUS).

\end{enumerate}




\subsection{Probability distribution}
\begin{enumerate} 
 
 \item (Probability distribution). Let $X:\ofp \to (Y, \G)$ be a random variable. The measure
 \[
  F_X(A) = \prob[X\in A] = \prob[\{\omega\in\Omega\mid X(\omega) \in A\}] = \prob[X^{-1}A] = (X_*\prob)(A),
 \]
 is called the \textit{probability distribution} of $X$ and it is a measure . Note that for all $A\in\G$, $X^{-1}A\in\F$
 since $X$ is measurable. 
 \item \label{rv221088}
 (Probability distribution of real-valued random variables).
 The \textit{probability distribution} or \textit{cumulative distribution function} of a random variable $X$ on a space
 $\mathcal{L}_p\ofp$ is $F_X(x) = \prob[X\leq x]$ for $x\in\Re$. The inverse cumulative
 distribution of $X$ is $F_X^{-1}(p)$ for $p\in[0,1]$ is defined as 
 $F_X^{-1}=\inf\{x\in\Re: F_X(x) \geq p\}$. 
 
 \item (Pushforward).
 \label{rv221089}
 The probability distribution of a random variable $X$ with values in $(\mathcal{X},\mathscr{G})$,
 is the pushforward measure $X_*\prob$ on $(\mathcal{X},\mathscr{G})$ which is 
 a probability measure on $(\mathcal{X},\mathscr{G})$ with $X_*\prob = \prob X^{-1}$.
 
 
 \item 
 \label{rv221137}
 We associate with $F_X:\Re\to[0,1]$ the measure $\mu$ which is defined on 
 the $p$-system $\{(-\infty,x]\}_{x\in\Re}$ as $\mu((-\infty, x]) = F_X(x)$.
 \item
 \label{rv231132}
 Properties of the cumulative and the inverse cumulative distributions. The notation
 $X\sim Y$ means that $X$ and $Y$ have the same cumulative distribution, that is 
 $F_X = F_Y$.
    \begin{enumerate}[i.]
      \item If $Y\sim U[0,1]$, then $F_X^{-1}(Y) \sim X$.
      \item $F_X$ is c\`adl\`ag
      \item $x_1<x_2 \Rightarrow F_X(x_1) \leq F_X(x_2)$
      \item $\prob[X>x] = 1 - F_X(x)$
      \item $\prob[\{x_1 < X \leq x_2\}] = F_X(x_2) - F_X(x_1)$
      \item $\lim_{x\to-\infty}F_X(x) = 0$, $\lim_{x\to\infty}F_X(x) = 1$
      \item $F_X^{-1}(F_X(x)) \leq x$
      \item $F_X(F_X^{-1}(p)) \geq p$
      \item $F_X^{-1}(p) \leq x \Leftrightarrow p \leq F_X(x)$
    \end{enumerate}
\end{enumerate}

\subsection{Probability density function}
\begin{enumerate}
 \item (Definition).
    The probability density function $f_X$ of a random variable $X:\ofp\to (\mathcal{X},\mathscr{G})$
    with respect to a measure $\mu$ on $(\mathcal{X},\mathscr{G})$ is the Radon-Nikodym derivative
    \[
     f_X = \frac{\d (X_*\prob)}{\d \mu},
    \]
    which exists provided that $X_*\prob \ll \mu$, and $f_X$ is measurable and $\mu$-integrable. Then,
    \begin{align*}
     \prob[X\in A] = \int_{X^{-1}A}\d \prob
                   = \int_{\Omega} 1_{X^{-1}A}\d\prob
                   = \int_{\Omega} (1_{A}\circ X)\d\prob
                   = \int_{A}\d(X_*\prob)
                   = \int_A f_X \d \mu.
    \end{align*}
  \item (Probability distribution).
	If $X$ is a real-valued random variable and its range ($\Re$) is taken with the 
        Borel $\sigma$-algebra, then 
        \begin{align*}
         \prob[X\leq x] = \int_{(-\infty, x]}X\d \prob
         = \int_{\{\omega\in\Omega: X(\omega) \leq x\}}\d \prob
         = \int_{-\infty}^x f_X\d \mu
        \end{align*}
        Note that the first integral is written with a slight abuse of notation as the 
        integration with respect to $\prob$ is carried out over the set $\{\omega\in\Omega: X(\omega) \leq x\}$;
        The first integral can be understood as shorthand notation for the second integral.
  \item (Expectation).
	Let a real-valued random variable $X$ have probability density $f_X$. Let $\iota$
	be the identity function $\iota:x\mapsto x$ on $\Omega$. Then
        \[
         \E[X] = \int_\Omega X\d\prob 
               = \int_\Omega (\iota\circ X)\d\prob 
               = \int_\Re \iota\d(X_*\prob)
               = \int_\Re \iota(x) f_X(x) \d\mu
               = \int_\Re x f_X(x) \d x.
        \]
  \item (Expectation of transformation).
\end{enumerate}

\subsection{Decomposition of measures}
Does a density function always exist? The answer is negative, but Lebesgue's decomposition 
theorem offers some further insight. 
\begin{enumerate}
 \item (Singular measures). Let $(\Omega, \F)$ be a measurable space and $\mu$, $\nu$
       be two measures defined thereon. These are called \textit{singular} if there are 
       $A,B\in\F$ so that
       \begin{enumerate}[i.]
        \item $A\cup B=\Omega$, 
        \item $A\cap B=\varnothing$,
        \item $\mu(B')=0$ for all $B'\in\F$ with $B'\subseteq B$,
        \item $\nu(A')=0$ for all $A'\in\F$ with $A'\subseteq A$.
       \end{enumerate}
 \item (Discrete measure on $\Re$). A measure $\mu$ on $\Re$ equipped with the Lebesgue $\sigma$-algebra,
       is said to be discrete if there is a (possibly finite) sequence of elements $\{s_k\}_{k\in\N}$,
       so that 
       \[
        \mu(\Re\setminus \bigcup_{k\in\N} \{s_k\}) = 0.
       \]
 \item (Lebesgue's decomposition Theorem). For every two $\sigma$-finite signed measures $\mu$ and $\nu$
       on a measurable space $(\Omega, \F)$, there exist two $\sigma$-finite signed measures $\nu_0$ and $\nu_1$ 
       on $(\Omega, \F)$ such that
       \begin{enumerate}[i.]
        \item $\nu = \nu_0 + \nu_1$
        \item $\nu_0\ll \mu$
        \item $\nu_1 {}\bot{} \mu$
       \end{enumerate}
       and $\nu_0$ and $\nu_1$ are uniquely determined by $\nu$ and $\mu$.
\item (Lebesgue's decomposition Theorem --- Corollary).
      Consider the space $(\Re,\B_\Re)$ and let $\mu$ be the Lebesgue measure. Any probability measure $\nu$
      on this space can be written as
      \[
       \nu = \nu_{\text{ac}} + \nu_{\text{sc}} + \nu_{\text{d}},
      \]
      where $\nu_{\text{ac}} \ll \mu$ (which is easily understood via the 
      Radon-Nikodym Theorem), $\nu_{\text{sc}}$ is singular continuous (wrt $\mu$) and $\nu_{\text{d}}$
      is a discrete measure.
             
\end{enumerate}

\subsection{Product spaces}
\begin{enumerate}
 \item (Product $\sigma$-algebra). Let $\{X_a\}_{a\in A}$ be an indexed collection of nonempty sets; define 
       $X=\prod_{a\in A}X_a$ and $\pi_a: X = (x_a)_{a\in A} \mapsto x_a\in X_a$. Let $\F_a$ be a $\sigma$-algebra
       on $X_a$. We define the product $\sigma$-algebra as
       \[
        \bigotimes_{a\in A} \F_a \dfn \sigma\left( \{\pi_a^{-1}(E_a);a\in A, E_a\in \F_a\}\right)
       \]
       This is the smallest $\sigma$-algebra on the product space which renders all projections measurable
       (compare to the definition of the \textit{product topology} which is the smallest topology on 
       the product space which renders the projections \textit{continuous}).
       
 \item (Measurability of epigraphs). Let $f:(X,\F)\to\barre$ be a measurable proper function. Its epigraph, that is
       the set $\epi f \dfn \{(x,\alpha)\in X\times \Re {}\mid{} f(x) \leq \alpha\}$ and its hypograph, that is
       the set $\hyp f \dfn \{(x,\alpha) \in X\times \Re {}\mid{} f(x) \geq \alpha\}$ are measurable in the product
       measure space $(X\times \Re, \F\otimes \B_\Re)$.
       
 \item (Measurability of graph). The graph of a measurable function $f:(X,\F,\mu)\to\Re$ is a Lebesgue-measurable set 
       with Lebesgue measure zero.
       
 \item (Countable product of $\sigma$-algebras). If $A$ is countable, the product $\sigma$-algebra       
       is generated by the products of measurable sets $\{\prod_{a\in A}E_a; E_a\in \F_a\}$.
       
 \item (Product measures). Let $(\mathcal{X},\F,\mu)$ and $(\mathcal{Y},\mathcal{G},\nu)$ be two measure spaces.
       The product space $\mathcal{X}\times \mathcal{Y}$ becomes a measurable space with the $\sigma$-algebra 
       $\F\otimes \mathcal{G}$. Let $E_x\in\F$ and $E_y\in\mathcal{G}$; then $E_x\times E_y\in\F\otimes \mathcal{G}$.
       We define a measure $\mu\times\nu$ on $(\mathcal{X}\times \mathcal{Y}, \F\otimes\mathcal{G})$ with 
       \[
        (\mu\times \nu)(E_x\times E_y) = \mu(E_x) \nu(E_y).
       \]
       
 \item Let $E\in \F\otimes\mathcal{G}$ and define 
       $E_x = \{y\in \mathcal{Y}: (x,y)\in E\}$ and $E_y = \{x\in \mathcal{X}: (x,y)\in E\}$.
       Then, $E_x\in \F$ for all $x\in\mathcal{X}$, $E_y\in\mathcal{G}$ for all $y\in\mathcal{Y}$.
 \item Let $f:\mathcal{X}\times\mathcal{Y}\to \Re$ be an $\F\otimes \mathcal{G}$-measurable function.
       Then, $f(x,\cdot)$ is $\mathcal{G}$-measurable for all $x\in\mathcal{X}$ and 
       $f(\cdot, y)$ is $\F$-measurable for all $y\in\mathcal{Y}$.
 \item Let $(\mathcal{X},\F,\mu)$ and $(\mathcal{Y},\mathcal{G},\nu)$ be two $\sigma$-finite measure spaces.
       For $E\in\F\otimes\mathcal{G}$, the mappings $\mathcal{X}\ni x\mapsto \nu(E_x) \in \Re$ and 
       $\mathcal{Y}\ni y\mapsto \mu(E_y)$ are measurable and
       \[
        (\mu\times \nu)(E) = \int \nu(E_x)\d \mu(x) = \int \mu(E_y)\d \nu(x)
       \]
 \item (Tonelli's Theorem). Let $h:\mathcal{X}\times \mathcal{Y}\to[0,\infty]$ be an $\F\otimes\mathcal{G}$-measurable
       function. Let
       \[
        f(x) = \int_{\mathcal{Y}} h(x,y) \d\nu(y), \ g(y) = \int_{\mathcal{X}}h(x,y)\d\mu(x).
       \]
       Then, $f$ and $g$ are measurable and 
       \[
        \int_{\mathcal{X}}f\d\mu = \int_{\mathcal{Y}}g\d\nu = \int_{\mathcal{X}\times\mathcal{Y}}g\d(\mu\times \nu).
       \]
 \item (Fubini's Theorem). 
       Let $h:\mathcal{X}\times \mathcal{Y}\to \Re$ be an $\F\otimes\mathcal{G}$-measurable
       function and
       \[
        \int_{\mathcal{X}} \int_{\mathcal{Y}} h(x,y)\d\nu(y) \d\mu(x) < \infty.
       \]
       Then, $h\in\mathcal{L}_1(\mathcal{X}\times\mathcal{Y}, \F\otimes\mathcal{G}, \mu\times\nu)$ and
        \[
        \int_{\mathcal{X}} \int_{\mathcal{Y}} h(x,y)\d\nu(y) \d\mu(x) = 
        \int_{\mathcal{Y}} \int_{\mathcal{X}} h(x,y)\d\mu(x) \d\nu(y) = 
        \int_{\mathcal{X}\times\mathcal{Y}} h \d(\mu\times \nu)
       \]
\end{enumerate}

\subsection{Transition Kernels}
\begin{enumerate}
 \item (Definition). Let $(X,\F)$, $(Y,\G)$ be two measurable spaces and let $K:X\times \G\to\barre$.
       $K$ is called a \textit{transition kernel} if
       \begin{enumerate}[i.]
        \item $f_B(x) \dfn K(x,B)$ is $\F$-measurable for every $B\in\G$,
        \item $\mu_x(B) \dfn K(x,B)$ is a measure on $(Y,\G)$ for every $x\in X$.
       \end{enumerate}
 \item (Existence of transition kernels). Let $\mu$ be a finite measure on $(X,\F)$ and $k:X\times Y\to\Re_+$
       be measurable in the product $\sigma$-algebra $\F\otimes \G$. Then,
       \[
        K(x,B) = \int_B k(x,y) \mu(\d y),
       \]
       is a transition kernel.
\end{enumerate}

\subsection{Law invariance}
\begin{enumerate}
 \item (Equality in distribution). 
      Let $X,Y$ be two real-valued random variables on $ofp$.
      We say that $X$ and $Y$ are equal in distribution, and we denote $X\overset{\mathrm{d}}{\sim} Y$,
      if $X$ and $Y$ have equal probability distribution functions, that is $F_X(s) = F_Y(s)$ for all $s$.

 \item (Equal in distribution, nowhere equal). Let $\Omega = \{-1,1\}$, $\F=2^\Omega$, $\prob[\{\omega_i\}]=\frac{1}{2}$.
       Let $X(\omega) = \omega$ and $Y(\omega) = -X(\omega)$. These two variables have the same distribution, but 
       are nowhere equal.
 
 \item (Equal in distribution, almost nowhere equal). Take $X\sim \mathcal{N}(0,1)$ and $Y=-X$. These 
       two random variables are almost nowhere equal, but have the same distribution.
       
 \item The following are equivalent:
      \begin{enumerate}[i.]
       \item $X\overset{\mathrm{d}}{\sim} Y$
       \item $\E[e^{-rX}]=\E[e^{-rY}]$ for all $r>0$
       \item $\E[f(X)] = \E[f(Y)]$ for all bounded continuous functions
       \item $\E[f(X)] = \E[f(Y)]$ for all bounded Borel functions
       \item $\E[f(X)] = \E[f(Y)]$ for all positive Borel functions	
      \end{enumerate}

 
 
\end{enumerate}


\section{Expectation}
\begin{enumerate}  

\item 
 \label{gx1312}
 Because of item~\ref{rv221030} in Sec.~\ref{sec:random_variables}, for $X\geq 0$ nonnegative 
 \begin{align*}
  \E[X] &= \int_{0}^{+\infty} X \d \prob\\
        &= \int_{0}^{+\infty} \int_0^{+\infty} 1_{X\geq t}\d t \d \prob\\
        &= \int_{0}^{+\infty} \int_0^{+\infty} 1_{X\geq t} \d \prob \d t
 \end{align*}
 and we use the fact that 
 \[
  \int_{0}^{+\infty} 1_{X>t}\d \prob = \prob[X>t],
 \]
 so
 \[
  \E[X] = \int_0^\infty \prob[X>t]\d t.
 \]
 The function $S(t) = \prob[X>t] = 1-\prob[X\leq t]$ is called the \textit{survival function} 
 of $X$, or its \textit{tail distribution} or \textit{exceedance}.

 \item Let $\ofp$ be a probability space and $X$ a real-valued random variable thereon. Define 
 \[
  f(\tau) = \int_{\Omega}(X-\tau)^2\d\prob.
 \]
 Then $\tau = \E[X]$ minimizes $f$ and the minimum value is $\mathrm{Var}[X]$.
 \item Let $X$ be a real-valued random variable. Then,
 \[
  \sum_{n=1}^{\infty}\prob[|X|\geq n] \leq \E[X] \leq 1+ \sum_{n=1}^{\infty}\prob[|X|\geq n].
 \]
 It is $\E[|X|]<\infty$ if and only if this series converges.
 \item If $X$ takes positive integer values, then
 \[
  \E[X] = \sum_{n=1}^{\infty}\prob[X\geq n]
 \]

 \item (Finite mean, infinite variance). There are several distributions with finite mean 
       and infinite variance --- a standard example is the \textit{Pareto distribution}.
       A random variable $X$ follows the Pareto distribution with parameters $x_m>0$ and $a$
       if it has support $[x_m,\infty)$ and probability distribution
       \[
        \prob[X\leq x] = \frac{ax_m^a}{x^{a+1}},
       \]
       for $x\geq x_m$. For $a\leq 1$, $X$ has infinite mean and variance. For $a>1$, its
       mean is $\E[X]=\frac{ax_m}{a-1}$ and infinite variance.
       
 \item (Absolute bounded a.s. $\Leftrightarrow$ Bounded moments)~[\ref{cite:Ambrosio2013}].
	Let $X$ be a random variable on $\ofp$. The following are equivalent:
	\begin{enumerate}[i.]
	  \item $X$ is almost surely absolutely bounded (i.e., $\prob[|X|\leq M]=1$)
	  \item $\E[|X|^k]\leq M^k$, for all $k\in \N_{\geq 1}$
	\end{enumerate}

 \item (A useful formula)~[\ref{cite:RLWolpert05}]. For $q>0$
 \[
  \E[|X|^q] = \int_0^\infty q x^{q-1} \prob[|X|>x]\d x.
 \]	

\end{enumerate}


\section{Conditional Expectation}
\begin{enumerate}
 \item (Conditional Expectation). Let $X$ be a random variable on $\ofp$ and $\HH\subseteq \F$.
       A \textit{conditional expectation} of $X$ given $\HH$ is an $\HH$-measurable 
       random variable, denoted as $\ce{X}$, with
       \[
        \int_H \ce{X} \d\prob = \int_H X\d\prob,
       \]
       which equivalently can be written as
       \[
        \E[X 1_H] = \E[\ce{X}1_H],
       \]
       for all $H\in\HH$.
 \item (Equivalent definition). It is equivalent to define the conditional expactation of $X$, 
       conditioned by a $\sigma$-algebra $\HH$ as a random variable $\ce{X}$ with the property
       \[
        \E[X Z] = \E[\ce{X}Z],
       \]
       for all $\HH$-measurable random variables $Z$.
 \item (Best estimator). Assuming $\E[Y^2]<\infty$, the best estimator of $Y$ given $X$ is $\E[Y{}\mid{}X]$      
 \item (Radon-Nikodym definition). The conditional expectation as introduced above, is the Radon-Nikodym
       derivative
       \[
          \ce{X} = \frac{\d \mu^X_{\HH}}{\d \prob_{\HH}},
       \]
      where $\mu^X_{\HH}:\HH\to [0,\infty]$ is the measure induced by $X$
      restricted on $\HH$, that is $\mu^X_{\HH}:H\mapsto \int_H X\d\prob$.
      This is absolutely continuous with respect to $\prob$. The measure $\prob_{\HH}$
      is the restriction of $\prob$ on $\HH$. 
      
 \item (Conditional expectation wrt random variable). Let $X,Y$ be random variables on $\ofp$.
       The conditional expectation of $X$ given $Y$ is $\E[X\mid Y]\dfn \E[X\mid \sigma(Y)]$,
       where $\sigma(Y)$ is the $\sigma$-algebra generated by $Y$, that is 
       $\sigma(Y) = Y^{-1}(\F) = \{Y^{-1}(B); B\in\F\}$.
       
 \item (Conditional expectation using the pushforward $Y_*\prob$). 
       Let $X$ be an integrable random variable on $\ofp$. Then, there is a $Y_*\prob$-unique 
       random variable $\E[X\mid Y]$
       \[
        \int_{Y^{-1}(B)} X\d \prob = \int_B \E[X{}\mid{}Y]\d(Y_*\prob).
       \]

 \item (Conditioning by an event). The conditional expectation $\E[X\mid H]$, conditioned
       by an event $H\in\F$ is given by
       \[
        \E[X\mid H] = \frac{1}{\prob[H]}\int_H X\d\prob = \frac{1}{\prob[H]}\E[X1_H].
       \]

 \item (Properties of conditional expectations). 
       The conditional expectation has the following properties:
       \begin{enumerate}[i.]
	\item \label{tp01} (Monotonicity). $X\leq Y \Rightarrow \ce{X} \leq \ce{Y}$
	\item (Positivity).  $X\geq 0 \Rightarrow \ce{X} \geq 0$ [Set $Y=0$ in~\ref{tp01}]. 
	\item (Linearity). For $a,b\in\Re$, $\ce{aX+bY}=a\ce{X} + b \ce{Y}$
	\item (Monotone convergence). $X_n\geq 0$, $X_n \uparrow X$ implies $\ce{X_n}\uparrow \ce{X}$
	\item (Fatou's lemma). For $X_n\geq 0$, $\ce{\liminf_n X_n}\leq \liminf_n \ce{X_n}$
	\item (Reverse Fatou's lemma).
	\item (Dominated convergence theorem). $X_n\to X$ (pointwise) and $|X_n|\leq Y$ $\prob$-a.s. where $Y$ is
	      integrable. Then, $\ce{X}$ is integrable and 
	      \[
	       \ce{X_n} \to \ce{X}.
	      \]
        \item (Jensen's inequality). Let $X\in\mathcal{L}_1\ofp$, $f:\Re\to\Re$ convex. Then
	      \[
	      f(\ce{X})\leq \ce{f(X)}.
	      \]
        \item (Law of total expectation). For any $\sigma$-algebra $\HH \subseteq \F$,
	      \[
	       \E[\ce{X}] = \E[X].
	      \]
        \item (Tower property). For two $\sigma$-algebras $\HH_1$ and $\HH_2$ with $\HH_1\subseteq \HH_2$,	      
	      \[
	       \E[\E[X {}\mid{} \HH_1] {}\mid{} \HH_2] = \E[\E[X {}\mid{} \HH_2] {}\mid{} \HH_1] = \E[X {}\mid{} \HH_1].
	      \]
        \item (Tower property with $X$ being $\HH_i$-measurable). Let $\HH_1\subseteq \HH_2$ be two $\sigma$-algebras. 
	      If $X$ is $\HH_1$-measurable, then it is also $\HH_2$-measurable.
	    

        \item If $X$ is $\HH$-measurable then
	      \[
	       \ce{X} = X.
	      \]

       \end{enumerate}
       %
       %
       % CHECK A LOT MORE INTERESTING RESULTS IN: ASH DOLEANS
       %
\end{enumerate}


\section{Inequalities on Probability Spaces}

\subsection{$\mathcal{L}_p$ spaces}
\begin{enumerate}
  \item (H\"older's inequality). If $X\in\mathcal{L}_p\ofp$, $Y\in\mathcal{L}_q\ofp$ (where $p$, $q$ are conjugate exponents), then $XY\in\mathcal{L}_1\ofp$ and \[ \E[|XY|] = \|XY\|_1 \leq \|X\|_p \|Y\|_q.\]
 
 \item (Cauchy-Schwarz inequality). This is H\"older's inequality with $p=q=2$:
 \[
    \|XY\|_1 \leq \|X\|_2 \|Y\|_2.
 \]

 \item (Minkowski inequality). If $X,Y\in\mathcal{L}_p\ofp$ ($p\in [1,\infty]$), then $X+Y\in\mathcal{L}_p\ofp$  and 
       $\|X+Y\|_p \leq \|X\|_p + \|Y\|_p$.
\end{enumerate}

\subsection{Generic inequalities involving probabilities or expectations}
\begin{enumerate}
  \item (Markov's inequality). Let $X\geq 0$, integrable. For all $t>0$, 
 \[ 
 \prob[X>t]\leq \frac{\E[X]}{t}.
 \]
 \item (Chebyshev's inequality). Let $X$ have finite expectation $\mu$ and finite variance $\sigma^2$. Then
 \[
  \prob[|X-\mu|\geq t] \leq \frac{\sigma^2}{t^2}.
 \]
 \item (Generalized Markov's inequality). Let $X$ be a real-valued random variable and $f:\Re\to\Re_+$
       be an increasing function. Then, for all $b\in\Re$,
       \[
        \prob[X>b]\leq \frac{1}{f(b)}\E[f(X)]
       \]
  \item (Gaussian tail inequality). Let $X\sim N(0,1)$. Then,
 \[ 
  \prob[|X|>\epsilon] \leq \frac{2e^{-\epsilon^2/2}}{\epsilon}.
 \]       
 \item (Hoeffding's lemma). Let $a\leq X\leq b$ be an RV with finite expectation $\mu=\E[X]$.
 Then
 \[
  \E[e^{tX}] \leq e^{t\mu}e^{\frac{t^2(b-a)^2}{8}}.
 \]
\item (Corollary of Hoeffding's lemma). Let $X$ be such that $e^{tX}$ is integrable for $t\geq 0$. Then
\[
 \prob[X>\epsilon]\leq \inf_{t\geq 0}e^{-t\epsilon}\E[e^{tX}].
\]
 \item (Jensen's inequality). Let $X\in\mathcal{L}_1\ofp$, $f:\Re\to\Re$ convex. Then
   \[
     f(\E[X])\leq \E[f(X)].
    \]
\item (Paley-Zygmund). Let $Z\geq 0$ be a random variable with finite variance. Then,
      \[
       \prob[Z > \theta \E[Z]] \geq (1-\theta)^2\frac{\E[Z]^2}{\E[Z^2]},
      \]
     and this bound can be improved (using the Cauchy-Schwartz inequality) as
     \[
       \prob[Z > \theta \E[Z]] \geq (1-\theta)^2\frac{\E[Z]^2}{\mathrm{Var}[Z]+(1-\theta)^2\E[Z^2]},
      \]
\item Let $X\geq 0$ and $\E[X^2]<\infty$. We apply the Cauchy-Schwarz inequality to $X1_{X>0}$ and obtain
      \[
       \prob[X>0] \geq \frac{\E[X]^2}{\E[X^2]}.
      \]



\item (Dvoretzky-Kiefer-Wolfowitz inequality). Let $X_1,\ldots, X_n$ be iid random variables (samples)
      with cumulative distribution $F$. Let $F_n$ be the associated empirical distribution
      \[
       F_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{X_i\leq x},
      \]
      Then,
      \[
       \prob[\sup_{x\in\Re}(F_n(x) - F(x)) > \epsilon] \leq e^{-2n\epsilon^2},
      \]
      for every $\epsilon \geq \sqrt{\tfrac{1}{2n}\ln 2}$.
\item (Chung-Erd\H{o}s inequality).          
\end{enumerate}

\subsection{Involving sums or averages}
\begin{enumerate}
 \item (Hoeffding's inequality for sums \#1). Let $X_1,X_2,\ldots, X_n$ be independent random variables in $[0,1]$. Define 
  \[
  \bar{X} = \frac{X_1 + X_2 + \ldots + X_n}{n}.
  \]
  Then,
  \[
  \prob[\bar{X} - \E[\bar{X}] \geq t] \leq e^{-2nt^2}.
  \]
\item (Hoeffding's inequality for sums \#2). Let $X_1,X_2,\ldots, X_n$ be independent random variables and $X_i\in [a_i, b_i]$.
  Let $\bar{X}$ be as above and let $r_i = b_i - a_i$. Then
  \[
  \prob[\bar{X} - \E[\bar{X}] \geq t] \leq \exp\left({-\frac{2n^2t^2}{\sum_{i=1}^{n} r_i^2}}\right),
  \]
  and
  \[
  \prob[ |\bar{X} - \E[\bar{X}]| \geq t] \leq 2 \exp\left({-\frac{2n^2t^2}{\sum_{i=1}^{n} r_i^2}}\right).
  \]
\item (Kolmogorov's inequality). Let $X_k$, $k=1,\ldots, N$ be independent random variables on $\ofp$
      with mean $0$ and variances $\sigma_k^2$. Let $S_k = X_1 + X_2 + \ldots + X_k$. For all $\epsilon>0$,
      \[
       \prob[\max_{1\leq k\leq n}|S_k|>\epsilon] \leq \frac{1}{\epsilon^2}\sum_{k=1}^{n}\sigma_k^2.
      \]
\item (Gaussian tail inequality for averages). Let $X_1,\ldots,X_n\sim \NN(0,1)$ and 
      let $\bar{X}_n \dfn n^{-1}\sum_{i=1}^{n}X_i$. Then $\bar{X}_n\sim \NN(0,n^{-1})$ and
      \[
       \prob[|\bar{X}_n|>\epsilon] \leq \frac{2e^{-n\epsilon^2/2}}{\sqrt{n}\epsilon}.
      \]
\item (Etemadi's inequality). Let $X_1,\ldots, X_n$ be independent real-valued random variables and $\alpha\geq 0$.
      Let $S_n = X_1 + \ldots + X_n$. Then
      \[
       \prob[\max_{1\leq i \leq n}|S_i|\geq 3\alpha]\leq \max_{1\leq i \leq n}\prob[|S_i|\geq \alpha].
      \]

\end{enumerate}


\section{Convergence of random processes}
\subsection{Convergence of measures}
\begin{enumerate}
 \item (Strong convergence). Let $\{\mu_k\}_{k\in\N}$ be a sequence of measures defined on a 
       measurable space $(\mathcal{X}, \mathscr{G})$. We say that the sequence converges strongly
       to a measure $\mu$ if
       \[
        \lim_k \mu_k(A) = \mu(A),
       \]
      for all $A\in\mathscr{G}$.
 \item (Total variation convergence). The total variation distance between two measures $\mu$ and 
       $\nu$ on a measurable space $(\mathcal{X}, \mathscr{G})$ is defined as
       \begin{align*}
        d_{\mathrm{TV}}(\mu,\nu) &= \|\mu-\nu\|_{\mathrm{TV}} \\
          &\dfn \sup \left\{\int_{\mathcal{X}}f\d\mu - \int_{\mathcal{X}}f \d \nu,\ f:\mathcal{X}\to[-1,1] \text{ measurable} \right\}\\
          &=2\sup_{A\in\mathscr{G}}|\mu(A) - \nu(A)|
       \end{align*}
      A sequence of measures $\{\mu_k\}_{k\in\N}$ converges in the total variation
      to a measure $\mu$ if $d_{\mathrm{TV}}(\mu_k(A)-\mu(A))\to 0$ as $k\to\infty$
      for all $A\in\mathscr{G}$.
 \item (Weak convergence). The sequence of measures $\{\mu_k\}_{k\in\N}$ is said to converge 
       in the weak sense, denoted by $\mu_k \rightharpoonup \mu$, if any of the conditions 
       of the \textit{Portmanteau Theorem} hold; these are
       \begin{enumerate}[i.]
        \item $\E_{\mu_k} f\to \E_{\mu} f$ for all bounded continuous functions $f$
        \item $\E_{\mu_k} f\to \E_{\mu} f$ for all bounded Lipschitz functions $f$
        \item $\limsup_k\E_{\mu_k} f \leq \E_{\mu} f$ for every upper semicontinuous $f$ bounded from above
        \item $\liminf_k\E_{\mu_k} f \geq \E_{\mu} f$ for every lower semicontinuous $f$ bounded from below
        \item $\limsup \mu_k(C) \leq \mu(C)$ for all closed set $C\subseteq \mathcal{X}$
        \item $\liminf \mu_k(O) \geq \mu(O)$ for all open set $O\subseteq \mathcal{X}$
       \end{enumerate}
 \item (Tightness). A sequence of measures $(\mu_n)_n$ is called \textit{tight} if for every $\epsilon>0$
       there is a compact set $K$ so that $\mu_n(K)>1-\epsilon$ for all $n\in\N$.
       
 \item (Prokhorov's Theorem). If $(\mu_n)_n$ is tight, then every subsequence of it has a further subsequence
       which is weakly convergent. 
       
 \item (L\'{e}vy-Prokhorov distance). Let $(X,d)$ be a metric space and let $\B_X$ be the Borel $\sigma$-algebra
       which makes $(X, \B_X)$ a measurable space. Let $\prob(X)$ be the space of all probability measures on 
       $(X, \B_X)$. For all $A\subseteq X$ we define
       \[
        A^\epsilon \dfn \{p \in X {}\mid{} \exists q\in A, d(p,q)<\epsilon\} = \bigcup_{p\in A} B_\epsilon(p),
       \]
       where $B_\epsilon(p)$ is an open ball centered at $p$ with radius $\epsilon$. 
       
       The L\'{e}vy-Prokhorov distance is a mapping $\pi:\prob(X)\times \prob(X)\to [0,1]$
       between two probability measures $\mu$ and $\nu$ defined as
       \[
        \pi(\mu,\nu) \dfn  \inf\{\epsilon>0 {}\mid{} \mu(A) \leq \nu(A^\epsilon)+\epsilon, \nu(A) \leq \mu(A^\epsilon)+\epsilon, \forall A\in \B_X\}.
       \]

 \item (Metrizability of weak convergence). If $(X, d)$ is a separable metric space, then convergence of a sequence of measures in the 
       L\'{e}vy-Prokhorov distance is equivalent to weak convergence.
       
 \item (Separability of $(\prob_X, \pi)$). The space $(\prob_X, \pi)$ is separable if and only if $(X,d)$ is separable.
       
 \item (Skorokhod's representation theorem). Let $(\mu_n)_n$ be a sequence of probability measures on a metric measurable space $(S,\HH)$
       such that $\mu_n\to \mu$ weakly. Suppose that the support of $\mu$ is separable\footnote{The support of a measure $\mu$
       on $\ofp$ which is equipped with a topology $\tau$ is the set of $\omega\in\Omega$ for which every open 
       neighbourhood $N_{\omega}$ of $\omega$ has a positive measure:
       $\mathrm{supp}(\mu)=\{\omega\in\Omega: \mu(N_x)>0, \text{ for all } N_\omega\in \tau, N_\omega\ni \omega\}$.}.
       Then, there exist random variables $(X_n)_n$ and $X$ on a common probability space such that the distribution 
       of $X_n$ is $\mu_n$, the distribution of $X$ is $\mu$ and $X_n\to X$ almost surely.
       
 \item (Strong $\nRightarrow$ TV).
\end{enumerate}

\subsection{Almost sure convergence}
\begin{enumerate}
 \item (Almost sure convergence). A sequence of random variables $(X_n)_n$ is said to converge \textit{almost surely}
       if the sequence $(X_n(\omega))_n$ converges (somewhere) for almost every $\omega$. It converges almost surely to $X$
       if $\lim_n X_n(\omega) = X(\omega)$ for almost every omega.
       
 \item (Uniqueness almost surely). If $X_n \to X$ a.s. and $X_n \to Y$ a.s., then $X=Y$ a.s.
 
 \item (Characterization of a.s. convergence). The sequence $(X_n)_n$ converges a.s. to $X$ if and only if for every $\epsilon>0$
       \[
        \sum_{n\in\N}1_{(\epsilon,\infty)}\circ|X_n - X| < \infty.
       \]

 \item (Characterization of a.s. convergence \textit{a l\`a} Borel-Cantelli \#1).
       The sequence $(X_n)_n$ converges a.s. to $X$ if for every $\epsilon>0$
       \[
        \sum_{n\in\N}\prob[|X_n-X|>\epsilon] < \infty.
       \]

 \item (Characterization of a.s. convergence \textit{a l\`a} Borel-Cantelli \#2).
       The sequence $(X_n)_n$ converges a.s. to $X$ if there is a decreasing sequence $(\epsilon_n)_n$
       converging to $0$ so that 
       \[
        \sum_{n\in\N}\prob[|X_n-X|>\epsilon_n] < \infty.
       \]
       
 \item (Cauchy criterion). The sequence $\{X_n\}_n$ is convergent almost surely if and only if
       $\lim_{m,n\to \infty}|X_n-X_m|\to 0$ almost surely.
       
 \item (Topological (non) characterization).
       The concept of almost sure convergence does not come from a topology on the space 
       of random variables. This means there is no topology on the space of random variables 
       such that the almost surely convergent sequences are exactly the converging sequences 
       with respect to that topology. In particular, there is no metric of almost sure convergence.
              
\end{enumerate}

\subsection{Convergence in probability}       
\begin{enumerate}
 \item (Convergence in probability). We say that the stochastic process $(X_n)_n$ converges to a random variable $X$
       in probability if for every $\epsilon>0$,
       \[
        \lim_n \prob[|X_n-X|>\epsilon] = 0.
       \]
       We denote $X_n \overset{p}{\to} X$.
 \item (Continuous mapping theorem). Let $X_n \overset{p}{\to} X$ and $g$ be a continuous mapping. Then
       $g(X_n) \overset{p}{\to} g(X)$.
 \item (Metrizability). Convergence in probability defines a topology which is metrizable via the \textit{Ky Fan metric}
      \[
	d(X,Y) = \inf \{\epsilon>0 {}\mid{} \prob[|X-Y|>\epsilon]\leq \epsilon\} = \E[\min(|X-Y|,1)].
      \]
 \item (Metrizability \#2). 
       The sequence $X_n$ converges to $0$ in probability if and only if
       \[
        \E\left[ \frac{|X_n|}{1+|X_n|}\right] \to 0.
       \]
       The functional 
       \[
        d(X,Y) \dfn \E\left[ \frac{|X - Y|}{1+|X - Y|}\right]
       \]
       is a metric that induces the convergence in probability (provided we identify two random variables
       as equal if they are almost everywhere equal).
 \item (Almost surely convergent subsequence). 
       If $X_n \overset{p}{\to} X$, then there exists a subsequence of $(X_n)_n$, 
       $(X_{k_n})_n$ which converges almost surely to $X$.
       
 \item (Sum of independent variables). Let $(X_n)_n$ be a sequence of independent random 
       variables and let $(S_n)_n$ be a sequence defined as $S_n = X_1 + \ldots + X_n$.
       Then $S_n$ converges almost surely if and only if it converges in probability.
       
 \item (Convergence of pairs). If $X_n\to X$ in probability and $Y_n\to Y$ in probability, then
       $(X_n, Y_n)\to (X,Y)$ in probability.
       
 \item (Almost surely $\Rightarrow$ in probability). If a sequence of random variables $\{X_k\}_k$ 
       converges almost surely, it converges in probability to the same limit.
       
 \item (In probability $\not\Rightarrow$ almost surely). 
       There are sequences which converge in probability but not almost surely. Here is an example:
              Let $(X_n)_n$ be a sequence of independent random variables on $\Omega=\N$
              with $X_n=1$ with probability $1/n$ and $0$ with probability $1-1/n$.
              Then, for any $\epsilon>0$ it is $\prob[|X_n|>\epsilon]=\tfrac{1}{n}\to 0$,
              but by the second Borel-Cantelli lemma since $\sum_{n=1}^{\infty}\prob[|X_n|>\epsilon]$
              (and the events $\{|X_n|>\epsilon\}$ are independent), we have $\prob[\limsup_n \{|X_n|>\epsilon\}]=1$.
       
\end{enumerate} 
 
\subsection{Convergence in $\mathcal{L}_p$}
\begin{enumerate}
 \item (Convergence in $\mathcal{L}_p\ofp$). 
 \item (Convergence $\mathcal{L}_1$ under uniform integrability). If $X_n \to X$ in probability and $(X_n)_n$ is uniformly
       integrable, then $X_n\to X$ in $\mathcal{L}_1$.
 \item (In $\mathcal{L}_s\ofp$ $\Rightarrow$ in $\mathcal{L}_p\ofp$, for $s>p\geq 1$). 
 \item (Convergence in $\mathcal{L}_p$ for all $p\in[1,\infty)$ but not in $\mathcal{L}_\infty$).
       Let $X$ be a random variable on $\Omega=\N$ which follows the Poisson distribution ($\prob[X=k]=\frac{e^{-\lambda}\lambda^k}{k!}$, $\lambda>0$).
       Define the sequence $X_k = 1_{\{X=k\}}$. Then $\|X_k\|_\infty=1$.
 \item (In $\mathcal{L}_p$ $\Rightarrow$ in probability).
 \item (Almost surely $\not\Rightarrow$ in $\mathcal{L}_p$). On $([0,1], \mathcal{B}_{[0,1]}, \lambda)$ take $X_n=n 1_{[0,1/n]}$.
       Then, for all $p\in[1,\infty]$ we have $\|X_n\|_p=1$, but the sequence converges almost surely to $0$.
\end{enumerate}


\subsection{Convergence in distribution}
\begin{enumerate}
\item (Convergence in distribution). The sequence of random variables $\{X_n\}_n$ with distributions 
      $\{\mu_n\}_n$ is said to converge in distribution of $X$ if $\{\mu_n\}_n$ converges weakly
      to $\mu$, the distribution of $X$.
      
\item (Slutsky's theorem). Let $X_k\to X$ in distribution and $Y_n\to c$ in probability, where $c$ is a constant.
      Then, 
      \begin{enumerate}[i.]
       \item $X_n + Y_n \to X + c$ in distribution
       \item $X_nY_n \to cX$ in distribution
       \item $X_n/Y_n \to X/c$ in distribution, provided that $c\neq 0$, $Y_n\neq 0$.
      \end{enumerate}
      
\item (Almost sure convergence). If $X_n\to X$ in distribution, we may find a probability space $\ofp$
      and random variables $Y$ and $(Y_n)_n$ so that $Y_n$ is equal in distribution to $X_n$,
      $Y$ is equal in distribution to $X$ and $Y_n\to Y$ almost surely.

\item (Convergence in probability $\Rightarrow$ in distribution). If $\{X_n\}_n$ converges in probability,
      then it converges in distribution to the same limit.
      
\item (In distribution $\not\Rightarrow$ in probability). There are sequences which converge in distribution,
      but not in probability. For example: On the space $([0,1], \mathcal{B}_{[0,1]},\lambda)$, let $X_{2n}(\omega)=\omega$
      and $X_{2n-1}(\omega) = 1-\omega$. Then all $X_k$ have the same distribution, but the sequence does not 
      converge in probability.

\end{enumerate}



\subsection{Tail events and 0-1 Laws}
\begin{enumerate}

 \item (Simple 0-1 law). Let $\{E_n\}$ be a sequence of independent events. Then $\prob[\limsup_n E_n]\in \{0,1\}$.
 \item (Unions of $\sigma$-algebras). Let $\F_1$, $\F_2$ be two $\sigma$-algebras on a nonempty set $X$.
       The $\sigma$-algebra generated by the sets $E_1\cup E_2$ with $E_1\in\F_1$ and $E_2\in\F_2$ is 
       denoted by $\F_1 \vee \F_2$
 
 \item (Tail $\sigma$-algebra). Let $(\F_n)_{n}$ be a sequence of sub-$\sigma$-algebras of $\F$.
       The $\sigma$-algebra $T_n\dfn \bigvee_{m>n}\F_m$ encodes the information about the future 
       after $n$ and $T=\bigcup_n T_n$ is the \textit{tail $\sigma$-algebra} which encodes the 
       information of the end of time. 
 
 \item (Kolmogorov's zero-one law). Let $(\F_n)_n$ be a sequence of \textit{independent}
       $\sigma$-algebras on a nonempty set $X$ and let $T$ be the tail $\sigma$-algebra.
       We equip $(X,\F)$ with a probability measure $\prob$. For every $H\in T$,
       $\prob(H)\in\{0,1\}$.
 
 \item (Borel-Cantelli lemma). Let $\{E_n\}_{n\in\N}$ be a sequence of events in $\ofp$. If
 \[
  \sum_{n\in\N}\prob[E_n] < \infty,
 \]
 Then,
 \[
  \prob[\limsup_n E_n] = 0.
 \]

 \item (Second Borel-Cantelli lemma). Let $\{E_n\}_{n\in\N}$ be a sequence of \textit{independent} events in $\ofp$. If
 \[
  \sum_{n\in\N}\prob[E_n] = \infty,
 \]
 Then
 \[
  \prob[\limsup_n E_n] = 1.
 \]
 \item (Counterpart of the Borel-Cantelli lemma). 
 Let $\{E_n\}_{n\in\N}$ be a nested increasing sequence of events in $\ofp$, that is 
 $E_k\subseteq E_{k+1}$ and let $E_k^c$ denote the complement of $E_k$.
 Infinitely many $E_k$ occur with probability $1$ if and only if there is an increasing sequence 
 $t_k\in\N$ such that
 \[
  \sum_k \prob[A_{t_{k+1}}\mid A_{t_k}^c]  = \infty.
 \] 
 
 \item (L\'evy's zero-one law). Let $\Ff=\{\F_k\}_{k\in\N}$ be any filtration of $\F$ on $\ofp$ and
 $X\in\mathcal{L}_1\ofp$. Let $\F_\infty$ be the minimum $\sigma$-algebra generated by $\Ff$. Then
 \[
  \E[X\mid \F_k] \to \E[X\mid \F_\infty], 
 \]
 both in $\mathcal{L}_1\ofp$ and $\prob$-a.s.
\end{enumerate}



\section{Stochastic Processes}
\subsection{General}
\begin{enumerate}
 \item (Stochastic process). Let $\T \subseteq \barre$ (e.g., $T=\N$ or $T=\barre$). A random process is 
       a sequence/net $(X_n)_{n\in \T}$ of random variables on a probability space $\ofp$.
       
 \item (Filtrations). A filtration is an increasing sequence of sub-$\sigma$-algebras of $\F$. The space 
       $(\Omega, \F, (\F_t)_{t\in \T}, \prob)$ is called a filtered probability space. The filtration 
       $\F_t = \sigma(\{X_s; s\in \T, s\leq t\})$ is called the filtration \textit{generated by $(X_n)_{n\in \T}$}.
       We say that $(X_n)_n$ is adapted to a filtration $(\F_n)_n$ if for all $n\in \T$, $X_n$ is $\F_n$-measurable.
       
 \item (Stopping times). Let $(\F_n)_n$ be a filtration on $\ofp$ and define $\overline{\T} \dfn \T \cup \{+\infty\}$.
       A random variable $T: \Omega \to \overline{\T}$ is called a stopping time if
       \[
        \{\omega {}\mid{} T(\omega) \leq t\} \in \F_t,
       \]
      for all $t\in \T$. This is equivalent to requiring that the process $Z_t = 1_{T \leq t}$ is adapted to $(\F_t)_{t\in \T}$.
      
 \item (A useful property). For any stochastic process $(X_n)_{n\in\N}$, we have
 \[
  \prob\left(\max_{i\leq k}|X_i|>\epsilon\right) = \prob\left(\sum_{i=0}^{k}X_i^2 \cdot  1_{\{|X_i|>\epsilon\}}>\epsilon^2\right).
 \]

\end{enumerate}

\subsection{Martingales}

\begin{enumerate}
 \item (Martingale). A random process $(X_n)_n$ is called a \textit{martingale} if 
		     $\E[|X_n|]<\infty$ and $\E[X_{n+1}{}\mid{}X_1,\ldots, X_n]=X_{n}$.
 \item (Martingale examples). The following are common examples of martingales:
  \begin{enumerate}
   \item Let $(X_n)_n$ be a sequence of iid random variables with mean $\E[X_n]=\mu$. Then
         $Y_n = \sum_{i=1}^{n}(X_i-\mu)$ is a martingale.
   \item If $(X_n)_n$ is a sequence of iid random variables with mean $1$, then 
         $Y_n = \prod_{i=1}^{n}X_i$ is a martingale.
   \item If $(X_n)_n$ is a sequence of random variables with finite expectation and 
         $\E[X_n{}\mid{}X_1,\ldots,X_{n-1}]=0$, then $Y_n=\sum_{i=0}^{n}X_i$ is a 
         martingale.
   \item (The classical martingale). The fortune of a gambler is a martingale in a fair game.
  \end{enumerate}
 \item (Sub- and super-martingales).
 \item (Stopping time).
 \item (Martingale stopping).
 \item (Kolmogorov's submartingale inequality).
 \item (Azuma-Hoeffding inequality for martingales with bounded differences). Let $(X_i)_i$
       be a martingale or a  supermartingale and $|X_k-X_{k-1}|<c_{k}$ almost surely. Then for all $N\in\N$
       and $t\in\Re$,
       \[
        \prob[X_N-X_0\geq t] \leq \exp \left(-\frac{t^2}{2\sum_{i=1}^{N}c_i^2}\right)
       \]
       If $(X_i)_i$ is a submartingale, 
       \[
        \prob[X_N-X_0\leq -t] \leq \exp \left(-\frac{t^2}{2\sum_{i=1}^{N}c_i^2}\right)
       \]

\end{enumerate}



\section{Information Theory}
\subsection{Entropy and Conditional Entropy}
\begin{enumerate}
 \item (Self-Information, construction). Let $\ofp$ be a discrete probability space. A self-information function $I$ must satisfy the 
       following desiderata: 
       (i) if $\omega_i$ is sure ($\prob[\omega_i]=1$), then this offers no information, that is $I(\omega_i)=0$, 
       (ii) if $\omega_i$ is not sure, that is $\prob[\omega_i]<1$, then $I(\omega_i)>0$,
       (iii) $I(\omega)$ depends on the probability $\prob[\omega]$, that is, there is a function $f$ so that $I(\omega)=f(\prob[\omega])$
       (iv) for two independent events $A$ and $B$, $I(A\cap B)=I(A)+I(B)$.
 \item (Self-information, definition). A definition which satisfied the above desiderata is $I(\omega)=-\log(\prob[\omega])$. 
 \item (Self-information, units). When $\log_2$ is used in the definition, the units of measurement of self-information are the \textit{bits}.
       If $\ln\equiv \log_e$ is used, the self-information is measures in \textit{nats}. 
       For the decimal logarithm, $I$ is measured in \textit{hartley}.
 \item (Entropy, definition). The \textit{entropy} (or Shannon entropy) of a random variable is the expectation of its self-information denoted 
       as $H(X)=\E[I(X)]$, where $I(X)$ is to be interpreted as follows: Let $\ofp$ be a probability space and $X:\ofp\to \{x_i\}_{i=1}^{n}$
       a finite-valued random variable. Consider the events $E_i=\{\omega\in\Omega {}\mid{} X(\omega) = x_i\}$ with self-information 
       $I(E_i)$. Then, $I(X)$ is the random variable $I(X)(\omega) = I(E_{\iota(\omega)})$, where $\iota(\omega)$ is such 
       that $X(\omega) = x_{\iota(\omega)}$.
       
       The entropy of $X$ is given by
       \[
        H(X) = -\sum_{i=1}^{n}p_i \log(p_i),
       \]
       where $p_i = \prob[X=x_i]$.
 \item (Joint entropy). The \textit{joint entropy} of two random variables $X$ and $Y$ (with values $\{x_i\}_i$ and $\{y_j\}_j$
       respectively) is the entropy of the random variable 
       $(X,Y)$ in the product space, that is
       \[
        H(X,Y) = -\sum_{i,j}p_{ij}\log p_{ij},
       \]
       where $p_{ij} = \prob[X=x_i, Y=y_j]$.
 \item (Conditional Entropy).
 \item (Mutual information).
\end{enumerate}

\subsection{KL divergence}
\begin{enumerate}
 \item (Definition).
 \item (Pinsker's inequality).
\end{enumerate}


%\section{Copulas}
%\lipsum[1]
\section{Paradoxes and Fun Facts}
\begin{enumerate}
 \item (Lebesgue measure of closure). In $\Re$, take the \textit{modified Cantor set} $C$. Then, $\lambda(E) < \lambda(\bar{E})$. 
 \item (Open cover of rationals, finite Lebesgue measure).
 \item (Product of integrable RVs, not integrable).
 \item ($\|f\|_\infty = \lim_p \|f\|_p$).
\end{enumerate}


\section{Bibliography with comments}
%
Bibliographic references including lecture notes and online resources with some comments:
{\small
\begin{enumerate}
 \item \label{cite:Ambrosio2013} \bibentry{Ambrosio2013}: A short note at \url{http://planetmath.org/sites/default/files/texpdf/38346.pdf}
       showing that almost surely bounded RVs have all their moments bounded.
 \item \label{cite:RLWolpert05} \bibentry{RLWolpert05}: Lecture notes with a succinct presentation of some very useful results,
        but without many proofs. Available at \url{https://www2.stat.duke.edu/courses/Spring05/sta205/lec/s05wk07.pdf}.
 \item \label{cite:cinlar2011} \bibentry{cinlar2011}: A fantastic book for one's first steps in probability theory with 
       emphasis on random processes, filtrations, Martingales, stopping times and convergence 
       theorems, Poisson random measures, L{\'{e}}vy and Markovian processes and Brownian motion.
 \item \label{cite:Kallnberg} \bibentry{Kallnberg}: The definitive reference for researchers. In its 23 chapters
       it gives a circumspect overview of probability theory and stochastic processes; ideal for researchers in the field.
 \item \label{cite:KSigman2009}\bibentry{KSigman2009}: Lecture notes by K. Sigman, Columbia University,
       \url{http://www.columbia.edu/~ks20/stochastic-I/stochastic-I.html}.
 \item \label{cite:DWalnut2011} \bibentry{DWalnut2011}: A short compilation of convergence theorems       
 \item \label{cite:Varadhan} \bibentry{Varadhan}: A lot of material on limit theorems starting from general measure theory,
       to weak convergence results, limits of independent sums, results for dependent processes 
       with emphasis on Markov chains, a comprehensive introduction to martingales, stationary processes
       and ergodic theorems and some notes on dynamic programming. Online at \url{https://www.math.nyu.edu/faculty/varadhan/}.
\end{enumerate}
}


\bibliographystyle{plain}
\nobibliography{bibliography} 
\end{document}
