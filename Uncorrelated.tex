\documentclass[a4paper,10pt]{article}
\usepackage{hyperref}

\usepackage{mathematix}
\usepackage[margin=1in,right=1in]{geometry}
\usepackage{lipsum}
\usepackage{enumerate}

\usepackage{caption}
\captionsetup{font=footnotesize,labelfont=bf}

\usepackage{natbib}
\usepackage{bibentry}
\nobibliography*

\renewcommand{\cov}{\operatorname{cov}}


\hypersetup{
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={Probability Theory Cookbook},    % title
    pdfauthor={Pantelis Sopasakis},     % author
    pdfsubject={},   % subject of the document
    pdfcreator={P. Sopasakis},   % creator of the document
    pdfproducer={P. Sopasakis}, % producer of the document
    pdfnewwindow=true,      % links in new PDF window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\newcommand{\ce}[1]{\E\left[#1 {}\mid{} \HH \right]}

% Update \bibentry so that bibentries appear in blue
\let\oldbibentry\bibentry
\renewcommand{\bibentry}[1]{{\color{blue} \oldbibentry{#1}}}

\title{Probability Cookbook}
\author{Pantelis Sopasakis}
\begin{document}
\maketitle
\tableofcontents

\begin{abstract}
 This document is intended to serve as a collection of important results in general probability
 theory and it can be used for a quick brush up or as a quick reference or 
 cheat sheet, but not as primary tutorial material.
\end{abstract}

\section{General Probability Theory}

\subsection{Measurable and Probability spaces}

\begin{enumerate}
 \item ($\sigma$-algebra). Let $X$ be a nonempty set. A collection $\F$ of subsets of $X$
       is called a $\sigma$-algebra if (i) $X \in \F$, (i) $A^c\in\F$ whenever $A\in\F$,
       (ii) if $A_1,\ldots, A_n\in\F$, then $\bigcup_{i=1,\ldots,n}A_i\in\F$. The space $X$
       equipped with a $\sigma$-algebra $\F$ is called a \textit{measurable space}.
       
 \item (d-system) A collection $\mathcal{D}$ of subsets of $X$ is called a d-system or a Dynkin class if 
       (i) $X\in\mathcal{D}$,
       (ii) $A\setminus B\in\mathcal{D}$ whenever $A,B\in\mathcal{D}$ and $A\supseteq B$,
       (iii) $A\in\mathcal{D}$ whenever $A_n\in \mathcal{D}$ and $A_n \uparrow A$ (meaning, 
       $A_{k}\subseteq A_{k+1}$ and $\bigcup_{k\in\N}A_k=A$).
       
 \item (p-system). A collection of sets $\mathcal{P}$ in $X$ is called a p-system
       if $A\cap B\in \mathcal{P}$ whenever $A,B\in\mathcal{P}$.
       
 \item A collection of sets is a $\sigma$-algebra if and only if it is both a p- and a d-system.
 
 \item (Smallest $\sigma$-algebra). Let $\HH$ be a collection of sets in $X$. The smallest collection of sets
       which contains $\HH$ and is a $\sigma$-algebra exists and is denoted by $\sigma(\HH)$.
 
 \item (Monotone class theorem). If a d-system $\mathcal{D}$ contains a p-system $\mathcal{P}$, then is also contains $\sigma(\mathcal{P})$.
 
 \item \label{mps1311949}
       (Borel $\sigma$-algebra). On $\Re$, the $\sigma$-algebra $\sigma(\{(a,b); a<b\})$ is called the Borel $\sigma$-algebra on $\Re$
       which we denote by $\B_\Re$. For topological spaces $(X,\tau)$, the Borel $\sigma$-algebra is
       defined as $\B_X = \sigma(\tau)$, i.e., it is the smallest $\sigma$-algebra which contains
       all open sets. $\B_\Re$ is generated by:
       \begin{enumerate}[i.]
        \item The open intervals $(a,b)$
        \item The closed intervals $[a,b]$
        \item All sets of the form $[a,b)$ or $(a,b]$
        \item Open rays $(a,\infty)$ or $(-\infty,a)$
        \item Closed rays $[a,\infty)$ or $(-\infty,a]$
       \end{enumerate}            

 \item (Measure). A function $\mu: \F\to [0,+\infty]$ is called a measure if 
       for every sequence of disjoint sets $A_n$ from $\F$, $\mu(\bigcup_n A_n)= \sum_n \mu(A_n)$.

 \item \label{mps1311960}
      (Properties of measures). The following hold:
      \begin{enumerate}[i.]
       \item (Empty set is negligible). $\mu(\varnothing)=0$ [Indeed, $\mu(A) = \mu(A\cup \varnothing) = \mu(A) + \mu(\varnothing)$ for all $A\in\F$]
       \item (Monotonicity). $A\subseteq B$ imples $\mu(A) \leq \mu(B)$ [Indeed, $\mu(B) = \mu(A\cup (B\setminus A))$]
       \item (Boole's inequality). For all $A_n\in\F$, $\mu(\bigcup_n A_n) \leq \sum_n \mu(A_n)$
       \item (Sequential continuity). If $A_n\uparrow A$, then $\mu(A_n)\uparrow \mu(A)$.
      \end{enumerate}

 \item (Equality of measures). Let $\mu,\nu$ be two measures on a measurable space $(X,\F)$ and let $\G$ 
       be a p-system generating $\F$. If $\mu(A) = \nu(A)$ for all $A\in \G$, then $\mu(B) = \nu(B)$
       for all $B\in\F$. As presented in \#\ref{mps1311949} above, p-systems are often available and 
       have simple forms.
      
 \item (Completeness). A measure space $(X,\F,\mu)$ is called \textit{complete} if the following holds:
	\[
	    A \in \F, \mu(A)=0, B\subseteq A \Rightarrow B \in \F.
	\]
       Of course, by the monotonicity property in \#\ref{mps1311960}--iii, if $(X,\F,\mu)$ is a complete 
       measure space then $\mu(B) = 0$.
       
 \item (Completion). Let $(X,\F,\mu)$ be a measure space and define the set of \textit{negligible sets} of $\mu$ as 
       $Z_\mu = \{N \subseteq X: \exists N'\supseteq N, N'\in\F \text{ s.t. } \mu(N')=0\}$.
       Let $\F'$ be the $\sigma$-algebra generated by $\F\cup Z_\mu$. Then
       \begin{enumerate}[i.]
        \item Every $B\in\F'$ can be written as $B=A\cup N$ with $A\in\F$ and $N\in Z_\mu$
        \item Define $\mu'(A\cup N) = \mu(A)$; this is a measure on $(X,\F')$ which renders 
              the space $(X,\F',\mu')$ complete. 
       \end{enumerate}
 
 \item (Lebesgue measure on $\Re$ and $\Re^n$). It suffices to define the \textit{Lebesgue measure} on $(\Re,\B_\Re)$
       on the p-system $\{(a,b), a<b\}$; it is $\lambda((a,b))=b - a$. This extends to a measure on  $(\Re,\B_\Re)$.
       Likewise, the collection of $n$-dimensional rectangles $\{(a_1, b_1)\times\ldots \times (a_n, b_n)\}$ is a p-system
       which generates $\B_{\Re^n}$; the Lebesgue measure on $(\Re^n, \B_{\Re^n})$ is 
       $\lambda(\prod_{i=1}^n (a_i, b_i))=\prod_{i=1}^n (b_i-a_i)$.
 
 \item (Lebesgue measurable sets). The completion of the Lebesgue measure defines the class of Lebesgue-measurable
       sets. 
             
 \item (Negligible boundary). If a set $C\subseteq \Re^n$ has a boundary whose Lebesgue measure is $0$, then 
       $C$ is Lebesgue measurable.
       
 \item (Independent events). Let $E_1,E_2$ be two events from $\ofp$; we say that $E_1$ and $E_2$ are \textit{independent}
       if $\prob[E_1\cap E_2] = \prob[E_1]\prob[E_2]$.
       
 \item (Independent $\sigma$-algebras). We say that two $\sigma$-algebras $\F_1$ and $\F_2$ on $\Omega$ 
       are independent if for any $E_1\in \F_1$ and $E_2\in\F_2$, $E_1$ and $E_2$ are independent.
       Note that $E_1\cap E_2$ is a member of the $\sigma$ algebra $E_1 \wedge E_2$.
       
 \item (Atom). Let $(\Omega, \F, \mu)$ be a measure space. A set $A\in\F$ is called an atom if
       $\mu(A)>0$ and for every $B\subset A$ with $\mu(B)<\mu(A)$ it is $\mu(B)=0$.
       A space without atoms is called nonatomic%
	    \footnote{A special class of spaces with (only) atoms are the discrete probability spaces where
	              $\F$ is generated by a discrete --- often finite --- set of events. Several 
	              results in measure theory require that the space be nonatomic. However, we may
	              often prove these results for discrete or finite spaces.}.
\end{enumerate}

\subsection{Random variables}\label{sec:random_variables}
\begin{enumerate}

 \item (Measurable function). A function $f:(X,\F)\to (Y,\G)$ (between two measurable spaces) is 
       called \textit{measurable} if $f^{-1}(G) \in \F$ for all $G\in\G$ (i.e., if it inverts all 
       measurable sets to measurables ones).
       
 \item (Measurability test). Let $\F,\G$ be $\sigma$-algebras on the nonempty sets $X$ and $Y$. Let $\G'$ be 
       a p-system which generates $\G$. A function $f: (X,\F)\to (Y,\G)$ is measurable 
       if and only if $f^{-1}(G')\in \F$ for all $G'\in\G'$ (it suffices to check the 
       measurability condition on a p-system).
       
 \item ($\sigma$-algebra generated by $f$). Let $f:(X,\F)\to (Y,\G)$ (between two measurable spaces) be a measurable
       function. The set 
       \[
        \sigma(f) \dfn \{f^{-1}(B){}\mid{} B\in \G\},
       \]
       is a sub-$\sigma$-algebra of $\F$ and is called the $\sigma$-algebra generated by $f$.
 
 \item (Sub/sup-level sets) Let $f:(X,\F)\to \Re$. The following are equivalent:
      \begin{enumerate}[i.]
       \item $f$ is measurable,
       \item Its \textit{sublevel sets}, that is
       sets of the form $\lev_{\leq \alpha} f \dfn \{x\in X: f(x) \leq \alpha\}$ are measurable,
	\item Its \textit{suplevel sets},
       that is sets of the form $\lev_{ \geq \alpha} f \dfn \{x\in X: f(x) \geq \alpha \}$ are 
       measurable. 
      \end{enumerate}

       
 \item \label{rv220000}
       (Random variable).
       A real-valued random variable $X:\ofp \to (\Re, \B_\Re)$ is a measurable function $X$
       from a probability space $\ofp$ to $\Re$, equipped with the Borel $\sigma$-algebra, that is, 
       for every Borel set $B$, $X^{-1}(B)\in\F$.              
       
 \item \label{rv221030}
      Every nonnegative (real-valued) random variable $X$ on $(\Re_+, \B_{{\Re}_+})$ 
      is written as 
      \[
        X(\omega) = \int_0^{+\infty} 1_{X(\omega)\geq t}\,\d t.
      \]

 \item (Increasing functions).  Every increasing function $f:\Re\to\barre$ is Borel-measurable.
 
 \item (Semicontinuous functions). Every lower semicontinuous function $X:\Omega\to\Re$ (where $\Omega$ is assumed 
       to be equipped with a topology) is Borel-measurable. 
       
 \item (Pushforward measure)~[\ref{cite:cinlar2011}]. Given measurable spaces $(\mathcal{X},\F)$ and $(\mathcal{Y}, \mathcal{G})$, 
 a measurable mapping $f: X \to Y$ and a (probability) measure $\mu$ on $(\mathcal{X},\F)$, the \textit{pushforward} of $\mu$
 is defined to be a measure $f∗(\mu)$ on $(\mathcal{Y}, \mathcal{G})$ given by
 \[
  (f_*\mu)(B) = \mu(f^{-1}(B)) = \mu(\{\omega\mid f(\omega)\in B\}),
 \]
 for $B\in\mathcal{G}$.
 \item (Change of variables). Let $F$ be a random variable on the probability space $\ofp$ and $F_*\prob$ 
 is the pushforward measure. random variable $X$ is integrable with respect to the pushforward measure $F_*\prob$
 if and only if $X\circ F$ is $\prob$-integrable. Then, the integrals coincide
 \[
  \int X \d(F_*\prob) = \int (X\circ F) \d \prob.
 \]
 \item (Measures from random variables). Let $X$ be a random variable on $\ofp$. 
       We may use $X$ to define the following measure
       \[
        \nu(A) = \int_A X\d \prob,
       \]
       defined for $A\in\F$. This is a positive measure which for short we denote as $\nu=X\prob$
       and it satisfies:
       \[
        \int_A Y\d \nu = \int_A XY\d \prob,
       \]
       for all random variables $Y$.
       
 \item (Compositions). Let $f:(X,\F_X)\to (Y, \F_Y)$ and $g:(Y,\F_Y)\to (Z,\F_Z)$ be two measurable functions. 
       Then, the function $h:(X,\F_X)\ni x\mapsto h(x) \dfn f(g(x)) \in (Z,\F_Z)$ is measurable. 
       
 \item (Simple function; definition). A simple function is one of the form
	\[
	 f(x) = \sum_{k=1}^{n}\alpha_k 1_{A_k}(x),
	\]
	where $1_{A_k}$ is the characteristic function of a measurable set $A_k$, that is
	\[
	 1_{A_k} = \begin{cases}
	            1,&\text{if }x\in A_{k}\\
	            0,&\text{otherwise}
	           \end{cases}
	\]



 \item (Characterization of measurability). A function $f:(X,\F)\to\Re$ is $\F$-measurable if and only if 
       it is the pointwise limit of a sequence of simple functions. A function $f:(X,\F)\to\Re_+$ is 
       $\F$-measurable if and only if  it is the pointwise limit of an increasing sequence of simple functions. 
       
 \item (Continuity and measurability). Every continuous function $f:(X,\F)\to\barre$ is Borel-measurable. 
  
 \item (Monotone class of functions). Let $M$ be a collection of functions $f:(X,\F)\to\barre$; let $M_+$
       be all positive functions in $M$ and $M_b$ all bounded functions in $M$. We say that $M$ is a \textit{monotone class}
       of functions if (i) $1\in M$, (ii) if $f,g\in M_b$ and $a,b\in \Re$, then $af+bg\in M$ and (iii)
       if $(f_n)_n\subseteq M_+$ and $f_n \uparrow f$, then $f\in M$.
       
 \item (Monotone class theorem for functions). Let $M$ be a monotone class of functions on $(X,\F)$. Suppose 
       that $\F$ is generated by some p-system $\mathcal{C}$, $1_A \in M$ for all $A\in\mathcal{C}$.
       Then, $M$ includes all positive $\F$-measurable functions and all bounded $\F$-measurable functions. 
       
 \item (Simple function approximation theorem).       
        Let $X$ be an extended-real-valued Lebesgue-measurable function defined on a Lebesgue measurable set $E$. 
        Then there exists a sequence $\{\phi_k\}_{k\in\N}$ of simple functions%
	    \footnote{A simple function is a finite linear combination of indicator functions of measurable sets, that is, 
	              simple functions are written as $\phi(x)=\sum_{i=1}^{n}\alpha_i 1_{A_i}(x)$. }
	 on $E$ such that
	\begin{enumerate}[i.]
	 \item $\phi_k\to X$, pointwise on $E$
	 \item $|\phi_k| \leq |X|$ on $E$ for all $k\in\N$
	\end{enumerate}
       If $X\geq 0$ then there exists a sequence of pointwise increasing simple functions with these properties.
       
  \item (Simple function approximation trick).         
        Let $f$ be a real-valued measurable function, $f:\ofp\to(\Re,\mathcal{B}_\Re)$. Define
        \[
         \phi_k(x) = \begin{cases}
                   \frac{j-1}{2^k},   &\frac{j-1}{2^k}\leq f(x) < \frac{j}{2^k}\\
                   k,                 &f(x) \geq k 
                  \end{cases}
        \]
        Then,
        \begin{enumerate}[i.]
         \item The sets $\{x: f(x) \geq k\}$ and $\{x: \frac{j-1}{2^k}\leq f(x) < \frac{j}{2^k}\}$ are measurable because $f$ is measurable
         \item $\phi_k$ are measurable for all $k\in\N$
         \item $\phi_k(x) \leq \phi_{k+1}(x)$ for all $k\in\N$ and for all $x\in\Omega$
         \item Let $E\subseteq \Omega$ so that $\sup_{x\in E}f(x) \leq M$. Then $\sup_{x\in\Omega}|f(x) - \phi_k(x)|\leq \nicefrac{1}{2^k}$ for all $k\geq M$
        \end{enumerate}

\end{enumerate}

\subsection{Limits}
\subsubsection{Limits of sequences of events}
\begin{enumerate}
 \item (Nested sequences and probabilities). Let $(E_n)_n$ be a nonincreasing sequence of events ($E_n\supseteq E_{n+1}$ for all $n\in\N$). 
       Then $\lim_n \prob[E_n]$ exists and 
       \[
        \prob\left[\bigcap_n E_n\right] = \lim_n \prob[E_n].
       \]
       If $(E_n)_n$ is a nondecreasing sequence ($E_n \subseteq E_{n+1}$ for all $n\in\N$), then
       \[
        \prob\left[\bigcup_n E_n\right] = \lim_n \prob[E_n].
       \]


 \item (Limits inferior). For a sequence of events $E_n$, the \textit{limit inferior} of $(E_n)_n$
       is denoted by $\liminf_n E_n$ and is defined as 
       \[
        \liminf_n E_n = \bigcup_{n\in \N}\bigcap_{m\geq n}E_n = \{x:\ x\in E_n\ \text{for all but finitely many } n\in \N\}.
       \]
 \item (Limit superior). The \textit{limit superior} of $(E_n)_n$, $\limsup_n E_n$, is
       \[
        \limsup_n E_n = \bigcap_{n\in \N}\bigcup_{m\geq n}E_n = \{x:\ x\in E_n\ \text{infinitely often} \}.
       \]
 \item (Limits of complements). The limit (super/inferior) of a sequence of complements is the complement of the limit
	\begin{align*}
	 \liminf_n E_n^c &= (\limsup_n E_n)^c,\\
	 \limsup_n E_n^c &= (\liminf_n E_n)^c.
	\end{align*}

 \item (Relationship between limits). It is
       \[
        \liminf_n E_n \subseteq \limsup_n E_n.
       \]
 \item (Probabilities of $\liminf E_n$ and $\limsup E_n$). The sets $\liminf_n E_n$ and $\limsup_n E_n$ are measurable and 
 \[
  \prob[\liminf_n E_n] \leq \liminf_n \prob[E_n] \leq \limsup_n \prob[E_n] \leq \prob[\limsup_n E_n].
 \]
 
 \item (A result reminiscent of Baire's category theorem). Let $(E_n)_n$ be a sequence of almost sure events. Then
       $\prob[\cap_n E_n] = 1$.

 \item (Borel-Cantelli lemma). Let $(E_n)_n$ be a sequence of events over $\ofp$. The following hold
 \begin{enumerate}[i.]
  \item If $\sum_{n=1}^{\infty}\prob[E_n]<\infty$, then $\prob[\limsup_n E_n] = 0$
  \item If $(E_n)_n$ are independent events such that $\sum_{n=1}^{\infty}\prob[E_n]=\infty$, then
        $\prob[\limsup_n E_n] = 1$.
 \end{enumerate}
 
 \item (Corollary: Borel 0-1 law). If $(E_n)_n$ is a sequence of independent events, 
       then $\prob[\limsup_n E_n]\in\{0,1\}$ (according to the summability of $(\prob[E_n])_n$).

 \item (Kochen-Stoone lemma). Let $(E_n)_n$ be a sequence of events. Then,
 \[
  \prob[\limsup_n E_n] \geq \limsup_n \frac{\left(\sum_{k=1}^n \prob[A_k]\right)^2}{\sum_{k=1}^{n}\sum_{j=1}^{n}\prob[A_k\cap A_j]}
 \]
 
 \item (Corollary of Kochen-Stoone's lemma). If for $i\neq j$, $E_i$ and $E_j$ are either independent 
       or $\prob[E_i\cap E_j] \leq \prob[E_i]\prob[E_j]$ and $\sum_{n=1}^{\infty}\prob[E_n]=\infty$,
       then $\prob[\limsup_n E_n] = 1$.

\end{enumerate}



\subsubsection{Limits of sequences of random variables}
\begin{enumerate}
 \item (Lebesgue's monotone convergence theorem). Let $(f_n)_n$ be an increasing sequence of 
       nonnegative Borel functions and let $f \dfn \lim_n f_n$ (in the sense $f_n\to f$ pointwise a.e.). 
       Then $\E[f_n] \uparrow \E[f]$.
      
 \item 	(Lebesgue's Dominated Convergence Theorem). Let $X_n$ be real-valued RVs over $\ofp$. 
	Suppose that $X_n$ converges pointwise to $X$ and is \textit{dominated} by a 
	$Y\in\mathcal{L}_1\ofp$, that is $|X_n|\leq Y$ $\prob$-a.s for all $n\in\N$. 
	Then, $X\in\mathcal{L}_1\ofp$
	and
	\[
	 \lim_n \E[|X_n-X|] = 0,
	\]
        which implies
        \[
         \lim_n \E[X_n] = \E[X].
        \]

 \item (Dominated convergence in $\mathcal{L}^p$).
       For $p\in[1,\infty)$ and a sequence of random variables $X_k:\ofp\to\barre$,
       assume that $X_k\to X$ almost everywhere ($X(\omega)=\lim_k X_k(\omega)$ $\prob$-a.e.)
       and there is $Y\in\mathcal{L}^p\ofp$ so that $X_k\leq Y$. Then, 
       \begin{enumerate}[i.]
        \item $X_k\in\mathcal{L}^p\ofp$ for all $k\in\N$,
        \item $X\in\mathcal{L}^p\ofp$ 
        \item $X_k\to X$ in $\mathcal{L}^p\ofp$, that is $\lim_k \|X_k-X\|_p = 0$.
       \end{enumerate}

 \item (Consequence of the dominated convergence theorem)~[\ref{cite:DWalnut2011}]. 
       Let $\{E_k\}_{k=1}^{\infty}$ be a collection of disjoint events and let $E=\bigcup_{k}E_k$.
       Then,
       \[
        \int_E f = \sum_{k=1}^{\infty} \int_{E_k} f.
       \]

 \item  (Bounded convergence). If $X_k \to X$ almost surely and $\sup_k |X_k| \leq b$
        for some constant $b>0$, then $\E[X_k]\to \E[X]$ and $\E[|X|] \leq b$.
       
 \item 	(Fatou's lemma). Let $X_n\geq 0$ be a sequence of random variables. 
	Then, 
	\[
	\E[\liminf_n X_n] \leq  \liminf_n \E[X_n].
	\]
 \item 	(Fatou's lemma with varying measures). For a sequence of nonnegative random variables $X_n\geq 0$ over $\ofp$,
	and a sequence of (probability) measures $\mu_n$ which converge strongly to a (probability)
	measure $\mu$ (that is, $\mu_n(A)\to\mu(A)$ for all $A\in\F$), we have
	\[
	 \E_\mu[\liminf_n X_n]\leq \liminf_n \E_{\mu_n}[ X_n]
	\]

 \item 	(Reverse Fatou's lemma). Let $X_n\geq 0$ be a sequence of nonnegative random variables over $\ofp$ and
	assume there is a $Y\in\mathcal{L}_1\ofp$ so that $X_n\leq Y$. Then
	\[ 
	 \limsup_n \E[X_n] \leq \E[\limsup_n X_n]
	\]
 \item (Integrable lower bound). 	
	Let $X_n$ be a sequence of random variables over $\ofp$. Suppose, there exists a
	$Y\geq 0$ such that $X_n\geq -Y$ for all $n\in\N$. Then,
	\[
	\E[\liminf_n X_n] \leq  \liminf_n \E[X_n].
	\]
 \item (Beppo Levi's Theorem).
	Let $X_k$ be a sequence of nonnegative random variables on $\ofp$ with $0 \leq X_1 \leq X_{2} \leq \ldots$. 
	Let $X(\omega) = \lim_{k\to\infty}X_k(\omega)$. Then $X$ is a random variable and 
	\[
	 \lim_{k\to\infty} \E[X_k] = \E[\lim_{k\to\infty} X_k].
	\]
 \item (Beppo Levi's Theorem for series).
       Let $X_k$ be a sequence of nonnegative integrable random variables on $\ofp$
       and let $Y_k = \sum_{j=0}^k X_k$. Assume that $\sum_{k=1}^{\infty} \E[Y_k]$ converges.
       Then $Y_k$ satisfies the conditions of the BL theorem and
       \[
        \sum_{k=1}^{\infty} \E[Y_k] = \E \left[\sum_{k=1}^{\infty}Y_k\right].
       \]

 \item (Uniform integrability -- definition)~[\ref{cite:KSigman2009}]. A collection $\{X_k\}_{k\in T}$ is said to be \textit{uniformly
        integrable} if $\sup_{t\in T}\E[|X_t| 1_{|X_t|>x}] \to 0$ as $x\to\infty$.
        
 \item (Constant absolutely integrable sequences as uniformly integrable)~[\ref{cite:KSigman2009}]. The sequence $\{Y\}_{t\in T}$
       with $\E[|Y|]<\infty$ is uniformly integrable.

 \item (Uniform boundedness in $\mathcal{L}^p$, $p>1$, implies uniform integrability).
       If $\{X_t\}_{t\in T}$ is uniformly bounded in $\mathcal{L}^p$, $p>1$ (that is, 
       $\E[|X_k|^p] < c$ for some $c>0$), then it is uniformly integrable.
       
 \item (Convergence under uniform integrability)~[\ref{cite:KSigman2009}]. If $X_k \to X$ a.s. and $\{X_k\}_k$ is uniformly 
       integrable then
       \begin{enumerate}[i.]
         \item $\E[X]< \infty$
         \item $\E[X_k] \to \E[X]$        
        \item $\E|X_k-X|\to 0$
       \end{enumerate}

\end{enumerate}


\subsection{The Radon-Nikodym Theorem}
\begin{enumerate}
 \item (Absolute continuity).
       Let $(\mathcal{X}, \mathscr{G})$ be a measurable space and $\mu$ and $\nu$ two measures on it.
       We say that $\nu$ is \textit{absolutely continuous} with respect to $\mu$ if
       for all $A\in\mathscr{G}$, $\nu(A)=0$ whenever $\mu(A)=0$. We denote this by $\nu\ll\mu$.
 \item (Radon-Nikodym). Let $(\mathcal{X}, \mathscr{G})$ be a measurable space, let $\nu$ be a \textit{$\sigma$-finite}
       measure on $(\mathcal{X}, \mathscr{G})$ which is {absolutely continuous} with respect 
       to a measure $\mu$ on $(\mathcal{X}, \mathscr{G})$. Then, there is a measurable function $f:\mathcal{X}\to[0,\infty)$
       such that for all $A\in \mathcal{G}$
       \[
        \nu(A) = \int_A f \d \mu.
       \]
      This function is denoted by $f=\frac{\d\nu}{\d \mu}$.
 \item (Linearity). Let $\nu$, $\mu$ and $\lambda$ be $\sigma$-finite measures on $(\mathcal{X}, \mathscr{G})$ and $\nu\ll\lambda$, $\mu\ll\lambda$.
       Then
       \[
        \frac{\d(\nu+\mu)}{\d \lambda} = \frac{\d \nu}{\d \lambda} + \frac{\d \mu}{\d \lambda},\ \lambda\text{-a.e.}
       \]
 \item (Chain rule). If $\nu\ll\mu\ll\lambda$,
 \[
  \frac{\d\nu}{\d\lambda} = \frac{\d\nu}{\d\mu} \frac{\d\mu}{\d\lambda},\ \lambda\text{-a.e.} 
 \]
 \item (Inverse). If $\nu\ll\mu$ and $\mu\ll\nu$, then
 \[
  \frac{\d \mu}{\d \nu} = \left( \frac{\d \nu}{\d \mu}\right)^{-1},\ \nu\text{-a.e.}
 \]
 \item (Change of measure).
 If $\mu\ll\lambda$ and $g$ is a $\mu$-integrable function, then
 \[
  \int_{\mathcal{X}} g \d \mu = \int_{\mathcal{X}} g \frac{\d \mu}{\d \lambda}\d \lambda.
 \]
 \item (Change of variables in integration). This was addressed using the pushforward. 
 \[
  \E[g(X)] = \int g\circ X\d\prob = \int_\Re g \,\d(X_*\prob).
 \]
 If the measure $\d(X_*\prob)$ is absolutely continuous with respect to the Lebesgue 
 measure $\mu$ (on $(\Re, \B_\Re)$, then, the Radon-Nikodym derivative $f_X\dfn \frac{\d(X_*\prob)}{\d \mu}$,
 where $f_X:\Re\to\Re$ exists. Then
 \[
  \E[g(X)] = \int_\Re g\, \d(X_*\prob) = \int_\Re g f_X\, \d\mu = \int_\Re g(\tau)f_X(\tau)\,\d \tau.
 \]
 This is known as the \textit{law of the unconscious statistician} (LotUS).

\end{enumerate}




\subsection{Probability distribution}
\begin{enumerate} 
 
 \item (Probability distribution). Let $X:\ofp \to (Y, \G)$ be a random variable. The measure
 \[
  F_X(A) = \prob[X\in A] = \prob[\{\omega\in\Omega\mid X(\omega) \in A\}] = \prob[X^{-1}A] = (X_*\prob)(A),
 \]
 is called the \textit{probability distribution} of $X$ and it is a measure . Note that for all $A\in\G$, $X^{-1}A\in\F$
 since $X$ is measurable. 
 \item \label{rv221088}
 (Probability distribution of real-valued random variables).
 The \textit{probability distribution} or \textit{cumulative distribution function} of a random variable $X$ on a space
 $\mathcal{L}^p\ofp$ is $F_X(x) = \prob[X\leq x]$ for $x\in\Re$. The inverse cumulative
 distribution of $X$ is $F_X^{-1}(p)$ for $p\in[0,1]$ is defined as 
 $F_X^{-1}=\inf\{x\in\Re: F_X(x) \geq p\}$. 
 
 \item (Pushforward).
 \label{rv221089}
 The probability distribution of a random variable $X$ with values in $(\mathcal{X},\mathscr{G})$,
 is the pushforward measure $X_*\prob$ on $(\mathcal{X},\mathscr{G})$ which is 
 a probability measure on $(\mathcal{X},\mathscr{G})$ with $X_*\prob = \prob X^{-1}$.
 
 
 \item 
 \label{rv221137}
 We associate with $F_X:\Re\to[0,1]$ the measure $\mu$ which is defined on 
 the $p$-system $\{(-\infty,x]\}_{x\in\Re}$ as $\mu((-\infty, x]) = F_X(x)$.
 \item
 \label{rv231132}
 Properties of the cumulative and the inverse cumulative distributions. The notation
 $X\sim Y$ means that $X$ and $Y$ have the same cumulative distribution, that is 
 $F_X = F_Y$.
    \begin{enumerate}[i.]
      \item If $Y\sim U[0,1]$, then $F_X^{-1}(Y) \sim X$.
      \item $F_X$ is c\`adl\`ag
      \item $x_1<x_2 \Rightarrow F_X(x_1) \leq F_X(x_2)$
      \item $\prob[X>x] = 1 - F_X(x)$
      \item $\prob[\{x_1 < X \leq x_2\}] = F_X(x_2) - F_X(x_1)$
      \item $\lim_{x\to-\infty}F_X(x) = 0$, $\lim_{x\to\infty}F_X(x) = 1$
      \item $F_X^{-1}(F_X(x)) \leq x$
      \item $F_X(F_X^{-1}(p)) \geq p$
      \item $F_X^{-1}(p) \leq x \Leftrightarrow p \leq F_X(x)$
    \end{enumerate}
\end{enumerate}

\subsection{Probability density function}
\begin{enumerate}
 \item (Definition).
    The probability density function $f_X$ of a random variable $X:\ofp\to (\mathcal{X},\mathscr{G})$
    with respect to a measure $\mu$ on $(\mathcal{X},\mathscr{G})$ is the Radon-Nikodym derivative
    \[
     f_X = \frac{\d (X_*\prob)}{\d \mu},
    \]
    which exists provided that $X_*\prob \ll \mu$, and $f_X$ is measurable and $\mu$-integrable. Then,
    \begin{align*}
     \prob[X\in A] = \int_{X^{-1}A}\d \prob
                   = \int_{\Omega} 1_{X^{-1}A}\d\prob
                   = \int_{\Omega} (1_{A}\circ X)\d\prob
                   = \int_{A}\d(X_*\prob)
                   = \int_A f_X \d \mu.
    \end{align*}
  \item (Probability distribution).
	If $X$ is a real-valued random variable and its range ($\Re$) is taken with the 
        Borel $\sigma$-algebra, then 
        \begin{align*}
         \prob[X\leq x] = \int_{(-\infty, x]}X\d \prob
         = \int_{\{\omega\in\Omega: X(\omega) \leq x\}}\d \prob
         = \int_{-\infty}^x f_X\d \mu
        \end{align*}
        Note that the first integral is written with a slight abuse of notation as the 
        integration with respect to $\prob$ is carried out over the set $\{\omega\in\Omega: X(\omega) \leq x\}$;
        The first integral can be understood as shorthand notation for the second integral.
  \item (Expectation).
	Let a real-valued random variable $X$ have probability density $f_X$. Let $\iota$
	be the identity function $\iota:x\mapsto x$ on $\Omega$. Then
        \[
         \E[X] = \int_\Omega X\d\prob 
               = \int_\Omega (\iota\circ X)\d\prob 
               = \int_\Re \iota\d(X_*\prob)
               = \int_\Re \iota(x) f_X(x) \d\mu
               = \int_\Re x f_X(x) \d x.
        \]
  \item (Distribution of transformation). Let $g:\Re\to\Re$ be a strictly increasing function. Let $X$ be a real-valued random variable with probability density function $f_X$ and let $Y(\omega) = g(X(\omega))$. Then
  \begin{align*}
   F_Y(y) &= F_X(g^{-1}(y)),\\
   f_Y(y) &= f_X(g^{-1}(y))\frac{\partial g^{-1}(y)}{\partial y}.
  \end{align*}

\end{enumerate}

\subsection{Decomposition of measures}
Does a density function always exist? The answer is negative, but Lebesgue's decomposition 
theorem offers some further insight. 
\begin{enumerate}
 \item (Singular measures). Let $(\Omega, \F)$ be a measurable space and $\mu$, $\nu$
       be two measures defined thereon. These are called \textit{singular} if there are 
       $A,B\in\F$ so that
       \begin{enumerate}[i.]
        \item $A\cup B=\Omega$, 
        \item $A\cap B=\varnothing$,
        \item $\mu(B')=0$ for all $B'\in\F$ with $B'\subseteq B$,
        \item $\nu(A')=0$ for all $A'\in\F$ with $A'\subseteq A$.
       \end{enumerate}
 \item (Discrete measure on $\Re$). A measure $\mu$ on $\Re$ equipped with the Lebesgue $\sigma$-algebra,
       is said to be discrete if there is a (possibly finite) sequence of elements $\{s_k\}_{k\in\N}$,
       so that 
       \[
        \mu(\Re\setminus \bigcup_{k\in\N} \{s_k\}) = 0.
       \]
 \item (Lebesgue's decomposition Theorem). For every two $\sigma$-finite signed measures $\mu$ and $\nu$
       on a measurable space $(\Omega, \F)$, there exist two $\sigma$-finite signed measures $\nu_0$ and $\nu_1$ 
       on $(\Omega, \F)$ such that
       \begin{enumerate}[i.]
        \item $\nu = \nu_0 + \nu_1$
        \item $\nu_0\ll \mu$
        \item $\nu_1 {}\bot{} \mu$
       \end{enumerate}
       and $\nu_0$ and $\nu_1$ are uniquely determined by $\nu$ and $\mu$.
\item (Lebesgue's decomposition Theorem --- Corollary).
      Consider the space $(\Re,\B_\Re)$ and let $\mu$ be the Lebesgue measure. Any probability measure $\nu$
      on this space can be written as
      \[
       \nu = \nu_{\text{ac}} + \nu_{\text{sc}} + \nu_{\text{d}},
      \]
      where $\nu_{\text{ac}} \ll \mu$ (which is easily understood via the 
      Radon-Nikodym Theorem), $\nu_{\text{sc}}$ is singular continuous (wrt $\mu$) and $\nu_{\text{d}}$
      is a discrete measure.
             
\end{enumerate}

\subsection{$\mathcal{L}^p$ spaces}
\begin{enumerate}
 \item ($p$-norm). Let $X$ be a real-valued random variable on $\ofp$. For $p\in[1,\infty)$ define the $p$-norm of $X$ as 
       \[
        \|X\|_p = \E[|X|^p]^{1/p}.
       \]
 \item ($\mathfrak{L}^p$ spaces). Define $\mathfrak{L}^p\ofp=\{X:\Omega\to\Re,\text{ measurable}, 
       \|X\|_p<\infty\}$ and equip this space with the addition and scalar multiplication 
       operations $(X+Y)(\omega) = X(\omega) + Y(\omega)$ and $(\alpha X)(\omega) = \alpha X(\omega)$.
       This becomes a seminormed space%
	  \footnote{$\|X\|=0$ does not imply that $X=0$, but instead 
		    that $X=0$ almost surely. However, $\|\cdot\|_p$ is absolutely 
		    homogeneous, subadditive and nonnegative}.%
 \item ($\mathcal{L}^p$ spaces). Define $\mathcal{N}\ofp = \{X:\Omega\to\Re,
       \text{ measurable}, X=0 \text{ a.s.}\}$; this is the kernel of $\|\cdot\|_p$. 
       Then, define $\mathcal{L}^p\ofp = \mathfrak{L}^p\ofp / \mathcal{N}$.
       This is a normed space where for $X\in\mathfrak{L}^p\ofp$ and $[X]=X+\mathcal{N}\in\mathcal{L}^p\ofp$
       we have $\|[X]\|_p \dfn \|X\|_p$.
 \item ($\infty$-norm, $\mathfrak{L}_\infty$ and $\mathcal{L}_\infty$). The infinity norm is defined as 
	\[
	 \|X\|_\infty = \esssup |X| = \inf\{\lambda \in \Re: \prob[|X|> \lambda] = 0\},
	\]
	or equivalently 
	\[
	 \|X\|_\infty = \inf \{\lambda \in \Re: |X|\leq \lambda,\, \prob\text{-a.s.}\}.
	\]
	The spaces $\mathfrak{L}_\infty\ofp$ and $\mathcal{L}_\infty\ofp$ are defined 
	similarly.
 \item ($\mathcal{L}_\infty\ofp$ as a limit). If there is a $p'\in [1,\infty)$ such 
       that $X\in\mathcal{L}_\infty\cap \mathcal{L}_{p'}$, then
       \[
        \|X\|_\infty = \lim_{p\to\infty}\|X\|_p.
       \]

 \item ($\mathcal{L}_2$ is a Hilbert space). $\mathcal{L}^p\ofp$ is the only Hilbert 
       $\mathcal{L}^p$ space with inner product
       \[
        \<X,Y\> = \E[XY].
       \]


\end{enumerate}


\subsection{Product spaces}
\begin{enumerate}
 \item (Product $\sigma$-algebra). Let $\{X_a\}_{a\in A}$ be an indexed collection of nonempty sets; define 
       $X=\prod_{a\in A}X_a$ and $\pi_a: X = (x_a)_{a\in A} \mapsto x_a\in X_a$. Let $\F_a$ be a $\sigma$-algebra
       on $X_a$. We define the product $\sigma$-algebra as
       \[
        \bigotimes_{a\in A} \F_a \dfn \sigma\left( \{\pi_a^{-1}(E_a);a\in A, E_a\in \F_a\}\right)
       \]
       This is the smallest $\sigma$-algebra on the product space which renders all projections measurable
       (compare to the definition of the \textit{product topology} which is the smallest topology on 
       the product space which renders the projections \textit{continuous}).
       
 \item (Measurability of epigraphs). Let $f:(X,\F)\to\barre$ be a measurable proper function. Its epigraph, that is
       the set $\epi f \dfn \{(x,\alpha)\in X\times \Re {}\mid{} f(x) \leq \alpha\}$ and its hypograph, that is
       the set $\hyp f \dfn \{(x,\alpha) \in X\times \Re {}\mid{} f(x) \geq \alpha\}$ are measurable in the product
       measure space $(X\times \Re, \F\otimes \B_\Re)$.
       
 \item (Measurability of graph). The graph of a measurable function $f:(X,\F,\mu)\to\Re$ is a Lebesgue-measurable set 
       with Lebesgue measure zero.
       
 \item (Countable product of $\sigma$-algebras). If $A$ is countable, the product $\sigma$-algebra       
       is generated by the products of measurable sets $\{\prod_{a\in A}E_a; E_a\in \F_a\}$.
       
 \item (Product measures). Let $(\mathcal{X},\F,\mu)$ and $(\mathcal{Y},\mathcal{G},\nu)$ be two measure spaces.
       The product space $\mathcal{X}\times \mathcal{Y}$ becomes a measurable space with the $\sigma$-algebra 
       $\F\otimes \mathcal{G}$. Let $E_x\in\F$ and $E_y\in\mathcal{G}$; then $E_x\times E_y\in\F\otimes \mathcal{G}$.
       We define a measure $\mu\times\nu$ on $(\mathcal{X}\times \mathcal{Y}, \F\otimes\mathcal{G})$ with 
       \[
        (\mu\times \nu)(E_x\times E_y) = \mu(E_x) \nu(E_y).
       \]
       
 \item Let $E\in \F\otimes\mathcal{G}$ and define 
       $E_x = \{y\in \mathcal{Y}: (x,y)\in E\}$ and $E_y = \{x\in \mathcal{X}: (x,y)\in E\}$.
       Then, $E_x\in \F$ for all $x\in\mathcal{X}$, $E_y\in\mathcal{G}$ for all $y\in\mathcal{Y}$.
 \item Let $f:\mathcal{X}\times\mathcal{Y}\to \Re$ be an $\F\otimes \mathcal{G}$-measurable function.
       Then, $f(x,\cdot)$ is $\mathcal{G}$-measurable for all $x\in\mathcal{X}$ and 
       $f(\cdot, y)$ is $\F$-measurable for all $y\in\mathcal{Y}$.
 \item Let $(\mathcal{X},\F,\mu)$ and $(\mathcal{Y},\mathcal{G},\nu)$ be two $\sigma$-finite measure spaces.
       For $E\in\F\otimes\mathcal{G}$, the mappings $\mathcal{X}\ni x\mapsto \nu(E_x) \in \Re$ and 
       $\mathcal{Y}\ni y\mapsto \mu(E_y)$ are measurable and
       \[
        (\mu\times \nu)(E) = \int \nu(E_x)\d \mu(x) = \int \mu(E_y)\d \nu(x)
       \]
 \item (Tonelli's Theorem). Let $h:\mathcal{X}\times \mathcal{Y}\to[0,\infty]$ be an $\F\otimes\mathcal{G}$-measurable
       function. Let
       \[
        f(x) = \int_{\mathcal{Y}} h(x,y) \d\nu(y), \ g(y) = \int_{\mathcal{X}}h(x,y)\d\mu(x).
       \]
       Then, $f$ and $g$ are measurable and 
       \[
        \int_{\mathcal{X}}f\d\mu = \int_{\mathcal{Y}}g\d\nu = \int_{\mathcal{X}\times\mathcal{Y}}g\d(\mu\times \nu).
       \]
 \item (Fubini's Theorem). 
       Let $h:\mathcal{X}\times \mathcal{Y}\to \Re$ be an $\F\otimes\mathcal{G}$-measurable
       function and
       \[
        \int_{\mathcal{X}} \int_{\mathcal{Y}} h(x,y)\d\nu(y) \d\mu(x) < \infty.
       \]
       Then, $h\in\mathcal{L}_1(\mathcal{X}\times\mathcal{Y}, \F\otimes\mathcal{G}, \mu\times\nu)$ and
        \[
        \int_{\mathcal{X}} \int_{\mathcal{Y}} h(x,y)\d\nu(y) \d\mu(x) = 
        \int_{\mathcal{Y}} \int_{\mathcal{X}} h(x,y)\d\mu(x) \d\nu(y) = 
        \int_{\mathcal{X}\times\mathcal{Y}} h \d(\mu\times \nu)
       \]
       
 \item (Consequence of Fubini's theorem). Let $X$ be a nonnegative random variable. Let $E=\{(\omega, x): 0 \leq x\leq X(\omega)\}$.
       Then, $X(\omega) = \int_{0}^{\infty}1_{E}(\omega, x)\d x$.
       \begin{align*}
        \E[X] = \int_{\Omega}X\d\prob &= \int_{\Omega}\int_{0}^{\infty}1_{E}(\omega, x)\d x\d \prob\\
                                      &= \int_{0}^{\infty} \int_{\Omega} 1_{E}(\omega, x)\d \prob \d x\\
                                      &= \int_{0}^{\infty} \prob[X\geq x]\d x.
       \end{align*}

\end{enumerate}

\subsection{Transition Kernels}
\begin{enumerate}
 \item (Definition). Let $(\mathcal{X},\F)$, $(\mathcal{Y},\G)$ be two measurable spaces and let $K:\G\times \mathcal{X}\to [0,1]$.
       $K$ is called a \textit{(probability) transition kernel} if
       \begin{enumerate}[i.]
        \item $f_B(x) \dfn K(B,x)$ is $\F$-measurable for every $B\in\G$,
        \item $\mu_x(B) \dfn K(B,x)$ is a measure on $(\mathcal{Y},\G)$ for every $x\in \mathcal{X}$.
       \end{enumerate}
 \item (Existence of transition kernels). Let $\mu$ be a finite measure on 
       $(\mathcal{X},\F)$ and $k:\mathcal{X}\times \mathcal{Y}\to\Re_+$
       be measurable in the product $\sigma$-algebra $\F\otimes \G$ and has the property $\int_{\mathcal{Y}} k(x,y)\nu(\d y)=1$. Then
       the mapping $K:\mathcal{X} \times \G \to [0,1]$ given by
       \[
        K(B,x) = \int_B k(x,y) \mu(\d y),
       \]
       is a probability transition kernel.
 \item (Measure on product space via a kernel). 
       Let $(\mathcal{X},\F)$, $(\mathcal{Y},\G)$ be two measurable spaces and let $K:\G\times\mathcal{X}\to [0,1]$
       be a transition kernel. For $A\in\F$ and $B\in\G$ define 
       \[
        \mu(A\times B) = \int_A K(B,x)\d\prob(x).
       \]
       This extends to a unique measure on the product space $(\mathcal{X}\times\mathcal{Y}, \F\otimes\G)$.
\end{enumerate}

\subsection{Law invariance}
\begin{enumerate}
 \item (Equality in distribution). 
      Let $X,Y$ be two real-valued random variables on $\ofp$.
      We say that $X$ and $Y$ are equal in distribution, and we denote $X\overset{\mathrm{d}}{\sim} Y$,
      if $X$ and $Y$ have equal probability distribution functions, that is $F_X(s) = F_Y(s)$ for all $s$.

 \item (Equal in distribution, nowhere equal). Let $\Omega = \{-1,1\}$, $\F=2^\Omega$, $\prob[\{\omega_i\}]=\frac{1}{2}$.
       Let $X(\omega) = \omega$ and $Y(\omega) = -X(\omega)$. These two variables have the same distribution, but 
       are nowhere equal.
 
 \item (Equal in distribution, almost nowhere equal). Take $X\sim \mathcal{N}(0,1)$ and $Y=-X$. These 
       two random variables are almost nowhere equal, but have the same distribution.
       
 \item The following are equivalent:
      \begin{enumerate}[i.]
       \item $X\overset{\mathrm{d}}{\sim} Y$
       \item $\E[e^{-rX}]=\E[e^{-rY}]$ for all $r>0$
       \item $\E[f(X)] = \E[f(Y)]$ for all bounded continuous functions
       \item $\E[f(X)] = \E[f(Y)]$ for all bounded Borel functions
       \item $\E[f(X)] = \E[f(Y)]$ for all positive Borel functions	
      \end{enumerate}

\end{enumerate}


\section{Expectation}
\begin{enumerate}  
\item (Definition)
 Let $\ofp$ be a probability space and $X$ be a random variable. Then, the expected value of 
 $X$ is denoted by $\E[X]$ and is defined as the Lebesgue integral
 \[
  \E[X] = \int_{\Omega} X\d \prob
 \]

\item 
 \label{gx1312}
 Because of item~\ref{rv221030} in Sec.~\ref{sec:random_variables}, for $X\geq 0$ nonnegative 
 \begin{align*}
  \E[X] &= \int_{0}^{+\infty} X \d \prob\\
        &= \int_{0}^{+\infty} \int_0^{+\infty} 1_{X\geq t}\d t \d \prob\\
        &= \int_{0}^{+\infty} \int_0^{+\infty} 1_{X\geq t} \d \prob \d t
 \end{align*}
 and we use the fact that 
 \[
  \int_{0}^{+\infty} 1_{X>t}\d \prob = \prob[X>t],
 \]
 so
 \[
  \E[X] = \int_0^\infty \prob[X>t]\d t.
 \]
 The function $S(t) = \prob[X>t] = 1-\prob[X\leq t]$ is called the \textit{survival function} 
 of $X$, or its \textit{tail distribution} or \textit{exceedance}.

 \item Let $\ofp$ be a probability space and $X$ a real-valued random variable thereon. Define 
 \[
  f(\tau) = \int_{\Omega}(X-\tau)^2\d\prob.
 \]
 Then $\tau = \E[X]$ minimizes $f$ and the minimum value is $\mathrm{Var}[X]$.
 \item Let $X$ be a real-valued random variable. Then,
 \[
  \sum_{n=1}^{\infty}\prob[|X|\geq n] \leq \E[|X|] \leq 1+ \sum_{n=1}^{\infty}\prob[|X|\geq n].
 \]
 It is $\E[|X|]<\infty$ if and only if the above series converges.
 \item If $X$ takes positive integer values, then
 \[
  \E[X] = \sum_{n=1}^{\infty}\prob[X\geq n]
 \]

 \item (Finite mean, infinite variance). There are several distributions with finite mean 
       and infinite variance --- a standard example is the \textit{Pareto distribution}.
       A random variable $X$ follows the Pareto distribution with parameters $x_m>0$ and $a$
       if it has support $[x_m,\infty)$ and probability distribution
       \[
        \prob[X\leq x] = \frac{ax_m^a}{x^{a+1}},
       \]
       for $x\geq x_m$. For $a\leq 1$, $X$ has infinite mean and variance. For $a>1$, its
       mean is $\E[X]=\frac{ax_m}{a-1}$ and infinite variance.
       
 \item (Absolutely bounded a.s. $\Leftrightarrow$ Bounded moments)~[\ref{cite:Ambrosio2013}].
	Let $X$ be a random variable on $\ofp$. The following are equivalent:
	\begin{enumerate}[i.]
	  \item $X$ is almost surely absolutely bounded (i.e., there is $M\geq 0$ such that $\prob[|X|\leq M]=1$)
	  \item $\E[|X|^k]\leq M^k$, for all $k\in \N_{\geq 1}$
	\end{enumerate}

 \item (A useful formula)~[\ref{cite:RLWolpert05}]. For $q>0$
 \[
  \E[|X|^q] = \int_0^\infty q x^{q-1} \prob[|X|>x]\d x.
 \]	

\end{enumerate}


\section{Conditioning}
\subsection{Conditional Expectation}
\begin{enumerate}
 \item (Conditional Expectation). Let $X$ be a random variable on $\ofp$ and $\HH\subseteq \F$.
       A \textit{conditional expectation} of $X$ given $\HH$ is an $\HH$-measurable 
       random variable, denoted as $\ce{X}$, with
       \[
        \int_H \ce{X} \d\prob = \int_H X\d\prob,
       \]
       which equivalently can be written as
       \[
        \E[X 1_H] = \E[\ce{X}1_H],
       \]
       for all $H\in\HH$.
 \item (Uniqueness). All versions of a conditional expectation, $\ce{X}$, differ only on a 
       set of measure zero%
       \footnote{R. Durrett, ``Probability: Theory and Examples,'' 2013, Available at: \url{https://services.math.duke.edu/~rtd/PTE/PTE4_1.pdf}}.
 \item (Equivalent definition). It is equivalent to define the conditional expactation of $X$, 
       conditioned by a $\sigma$-algebra $\HH$ as a random variable $\ce{X}$ with the property
       \[
        \E[X Z] = \E[\ce{X}Z],
       \]
       for all $\HH$-measurable random variables $Z$.
 \item (Best estimator). Assuming $\E[Y^2]<\infty$, the best estimator of $Y$ given $X$ is $\E[Y{}\mid{}X]$      
 \item (Radon-Nikodym definition). The conditional expectation as introduced above, is the Radon-Nikodym
       derivative
       \[
          \ce{X} = \frac{\d \mu^X_{\HH}}{\d \prob_{\HH}},
       \]
      where $\mu^X_{\HH}:\HH\to [0,\infty]$ is the measure induced by $X$
      restricted on $\HH$, that is $\mu^X_{\HH}:H\mapsto \int_H X\d\prob$.
      This is absolutely continuous with respect to $\prob$. The measure $\prob_{\HH}$
      is the restriction of $\prob$ on $\HH$. 
      
 \item (Conditional expectation wrt random variable). Let $X,Y$ be random variables on $\ofp$.
       The conditional expectation of $X$ given $Y$ is $\E[X\mid Y]\dfn \E[X\mid \sigma(Y)]$,
       where $\sigma(Y)$ is the $\sigma$-algebra generated by $Y$, that is 
       $\sigma(Y) = Y^{-1}(\F) = \{Y^{-1}(B); B\in\F\}$.
       
 \item (Conditional expectation using the pushforward $Y_*\prob$). 
       Let $X$ be an integrable random variable on $\ofp$. Then, there is a $Y_*\prob$-unique 
       random variable $\E[X\mid Y]$
       \[
        \int_{Y^{-1}(B)} X\d \prob = \int_B \E[X{}\mid{}Y]\d(Y_*\prob).
       \]

 \item (Conditioning by an event). The conditional expectation $\E[X\mid H]$, conditioned
       by an event $H\in\F$ is given by
       \[
        \E[X\mid H] = \frac{1}{\prob[H]}\int_H X\d\prob = \frac{1}{\prob[H]}\E[X1_H].
       \]

 \item (Properties of conditional expectations). 
       The conditional expectation has the following properties:
       \begin{enumerate}[i.]
	\item \label{tp01} (Monotonicity). $X\leq Y \Rightarrow \ce{X} \leq \ce{Y}$
	\item (Positivity).  $X\geq 0 \Rightarrow \ce{X} \geq 0$ [Set $Y=0$ in~\ref{tp01}]. 
	\item (Linearity). For $a,b\in\Re$, $\ce{aX+bY}=a\ce{X} + b \ce{Y}$
	\item (Monotone convergence). $X_n\geq 0$, $X_n \uparrow X$ implies $\ce{X_n}\uparrow \ce{X}$
	\item (Fatou's lemma). For $X_n\geq 0$, $\ce{\liminf_n X_n}\leq \liminf_n \ce{X_n}$
	\item (Reverse Fatou's lemma).
	\item (Dominated convergence theorem). $X_n\to X$ (pointwise) and $|X_n|\leq Y$ $\prob$-a.s. where $Y$ is
	      integrable. Then, $\ce{X}$ is integrable and 
	      \[
	       \ce{X_n} \to \ce{X}.
	      \]
        \item (Jensen's inequality). Let $X\in\mathcal{L}_1\ofp$, $f:\Re\to\Re$ convex. Then
	      \[
	      f(\ce{X})\leq \ce{f(X)}.
	      \]
        \item (Law of total expectation). For any $\sigma$-algebra $\HH \subseteq \F$,
	      \[
	       \E[\ce{X}] = \E[X].
	      \]
        \item (Tower property). For two $\sigma$-algebras $\HH_1$ and $\HH_2$ with $\HH_1\subseteq \HH_2$,	      
	      \[
	       \E[\E[X {}\mid{} \HH_1] {}\mid{} \HH_2] = \E[\E[X {}\mid{} \HH_2] {}\mid{} \HH_1] = \E[X {}\mid{} \HH_1].
	      \]
        \item (Tower property with $X$ being $\HH_i$-measurable). Let $\HH_1\subseteq \HH_2$ be two $\sigma$-algebras. 
	      If $X$ is $\HH_1$-measurable, then it is also $\HH_2$-measurable.
	    

        \item If $X$ is $\HH$-measurable then
	      \[
	       \ce{X} = X.
	      \]

       \end{enumerate}
       %
       %
       % CHECK A LOT MORE INTERESTING RESULTS IN: ASH DOLEANS
       %
\end{enumerate}


\subsection{Conditional Probability}
\begin{enumerate}
 \item (Conditional probability). Let $\ofp$ be a probability space and $\HH$ be a sub-$\sigma$-algebra
       of $\F$. We define $\prob_{\HH}$ as an operator so that for all $H\in\HH$
       \[
        \prob_{\HH}[H] = \E_{\F}1_{H}.
       \]
 \item (Conditional probability given an event). For $E,H\in\F$, $\prob[E\cap H] = \prob[H]\prob_{H}[E]$. This is
       uniquely defined provided that $\prob[H]>0$.
\end{enumerate}

\subsection{Construction of probability spaces}
\begin{enumerate}
 \item (Inonescu-Tulcea's Theorem). 
 \item (Kolmogorov's Extension Theorem). 
\end{enumerate}



\section{Inequalities on Probability Spaces}

\subsection{Inequalities on $\mathcal{L}^p$ spaces}
\begin{enumerate}
  \item (H\"older's inequality). If $X\in\mathcal{L}^p\ofp$, $Y\in\mathcal{L}_q\ofp$ (where $p$, $q$ are conjugate exponents), then $XY\in\mathcal{L}_1\ofp$ and \[ \E[|XY|] = \|XY\|_1 \leq \|X\|_p \|Y\|_q.\]
 
 \item (Cauchy-Schwarz inequality). This is H\"older's inequality with $p=q=2$:
 \[
    \|XY\|_1 \leq \|X\|_2 \|Y\|_2.
 \]

 \item (Minkowski inequality). If $X,Y\in\mathcal{L}^p\ofp$ ($p\in [1,\infty]$), then $X+Y\in\mathcal{L}^p\ofp$  and 
       $\|X+Y\|_p \leq \|X\|_p + \|Y\|_p$.
\end{enumerate}

\subsection{Generic inequalities involving probabilities or expectations}
\begin{enumerate}
  \item (Lyapunov's inequality). Let $0<s<t$. Then
  \[
   (\E[|X|^s])^{\nicefrac{1}{s}} \leq (\E[|X|^t])^{\nicefrac{1}{t}}.
  \]

  \item (Markov's inequality). Let $X\geq 0$, integrable. For all $t>0$, 
 \[ 
 \prob[X>t]\leq \frac{\E[X]}{t}.
 \]
 \item (Chebyshev's inequality). Let $X$ have finite expectation $\mu$ and finite variance $\sigma^2$. Then
 \[
  \prob[|X-\mu|\geq t] \leq \frac{\sigma^2}{t^2}.
 \]
 \item (Generalized Markov's inequality). Let $X$ be a real-valued random variable and $f:\Re\to\Re_+$
       be an increasing function. Then, for all $b\in\Re$,
       \[
        \prob[X>b]\leq \frac{1}{f(b)}\E[f(X)]
       \]
  \item (Gaussian tail inequality). Let $X\sim N(0,1)$. Then,
 \[ 
  \prob[|X|>\epsilon] \leq \frac{2e^{-\epsilon^2/2}}{\epsilon}.
 \]       
 \item (Hoeffding's lemma). Let $a\leq X\leq b$ be an RV with finite expectation $\mu=\E[X]$.
 Then
 \[
  \E[e^{tX}] \leq e^{t\mu}e^{\frac{t^2(b-a)^2}{8}}.
 \]
\item (Corollary of Hoeffding's lemma). Let $X$ be such that $e^{tX}$ is integrable for $t\geq 0$. Then
\[
 \prob[X>\epsilon]\leq \inf_{t\geq 0}e^{-t\epsilon}\E[e^{tX}].
\]
 \item (Jensen's inequality). Let $X\in\mathcal{L}_1\ofp$, $f:\Re\to\Re$ convex. Then
   \[
     f(\E[X])\leq \E[f(X)].
    \]
\item (Paley-Zygmund). Let $Z\geq 0$ be a random variable with finite variance. Then,
      \[
       \prob[Z > \theta \E[Z]] \geq (1-\theta)^2\frac{\E[Z]^2}{\E[Z^2]},
      \]
     and this bound can be improved (using the Cauchy-Schwartz inequality) as
     \[
       \prob[Z > \theta \E[Z]] \geq (1-\theta)^2\frac{\E[Z]^2}{\mathrm{Var}[Z]+(1-\theta)^2\E[Z^2]},
      \]
\item Let $X\geq 0$ and $\E[X^2]<\infty$. We apply the Cauchy-Schwarz inequality to $X1_{X>0}$ and obtain
      \[
       \prob[X>0] \geq \frac{\E[X]^2}{\E[X^2]}.
      \]



\item (Dvoretzky-Kiefer-Wolfowitz inequality). Let $X_1,\ldots, X_n$ be iid random variables (samples)
      with cumulative distribution $F$. Let $F_n$ be the associated empirical distribution
      \[
       F_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{X_i\leq x},
      \]
      Then,
      \[
       \prob[\sup_{x\in\Re}(F_n(x) - F(x)) > \epsilon] \leq e^{-2n\epsilon^2},
      \]
      for every $\epsilon \geq \sqrt{\tfrac{1}{2n}\ln 2}$.
\item (Chung-Erd\H{o}s inequality). Let $E_1,\ldots, E_n\in\F$ and $\prob[E_i]>0$ for some $i$. Then 
      \[
       \prob[E_1 \vee \ldots \vee E_n] \geq \frac{(\sum_{i=1}^{n}\prob[E_i])^2}{\sum_{i=1}^{n}\sum_{j=1}^{n}\prob[E_i\wedge E_j]}
      \]

\end{enumerate}

\subsection{Involving sums or averages}
\begin{enumerate}
 \item (Hoeffding's inequality for sums \#1). Let $X_1,X_2,\ldots, X_n$ be independent random variables in $[0,1]$. Define 
  \[
  \bar{X} = \frac{X_1 + X_2 + \ldots + X_n}{n}.
  \]
  Then,
  \[
  \prob[\bar{X} - \E[\bar{X}] \geq t] \leq e^{-2nt^2}.
  \]
\item (Hoeffding's inequality for sums \#2). Let $X_1,X_2,\ldots, X_n$ be independent random variables and $X_i\in [a_i, b_i]$.
  Let $\bar{X}$ be as above and let $r_i = b_i - a_i$. Then
  \[
  \prob[\bar{X} - \E[\bar{X}] \geq t] \leq \exp\left({-\frac{2n^2t^2}{\sum_{i=1}^{n} r_i^2}}\right),
  \]
  and
  \[
  \prob[ |\bar{X} - \E[\bar{X}]| \geq t] \leq 2 \exp\left({-\frac{2n^2t^2}{\sum_{i=1}^{n} r_i^2}}\right).
  \]
\item (Kolmogorov's inequality). Let $X_k$, $k=1,\ldots, N$ be independent random variables on $\ofp$
      with mean $0$ and variances $\sigma_k^2$. Let $S_k = X_1 + X_2 + \ldots + X_k$. For all $\epsilon>0$,
      \[
       \prob[\max_{1\leq k\leq n}|S_k|>\epsilon] \leq \frac{1}{\epsilon^2}\sum_{k=1}^{n}\sigma_k^2.
      \]
\item (Gaussian tail inequality for averages). Let $X_1,\ldots,X_n\sim \NN(0,1)$ and 
      let $\bar{X}_n \dfn n^{-1}\sum_{i=1}^{n}X_i$. Then $\bar{X}_n\sim \NN(0,n^{-1})$ and
      \[
       \prob[|\bar{X}_n|>\epsilon] \leq \frac{2e^{-n\epsilon^2/2}}{\sqrt{n}\epsilon}.
      \]
\item (Etemadi's inequality). Let $X_1,\ldots, X_n$ be independent real-valued random variables and $\alpha\geq 0$.
      Let $S_n = X_1 + \ldots + X_n$. Then
      \[
       \prob[\max_{1\leq i \leq n}|S_i|\geq 3\alpha]\leq \max_{1\leq i \leq n}\prob[|S_i|\geq \alpha].
      \]

\end{enumerate}


\section{Convergence of random processes}
\subsection{Convergence of measures}
\begin{enumerate}
 \item (Strong convergence). Let $\{\mu_k\}_{k\in\N}$ be a sequence of measures defined on a 
       measurable space $(\mathcal{X}, \mathscr{G})$. We say that the sequence converges strongly
       to a measure $\mu$ if
       \[
        \lim_k \mu_k(A) = \mu(A),
       \]
      for all $A\in\mathscr{G}$.
 \item (Total variation convergence). The total variation distance between two measures $\mu$ and 
       $\nu$ on a measurable space $(\mathcal{X}, \mathscr{G})$ is defined as
       \begin{align*}
        d_{\mathrm{TV}}(\mu,\nu) &= \|\mu-\nu\|_{\mathrm{TV}} \\
          &\dfn \sup \left\{\int_{\mathcal{X}}f\d\mu - \int_{\mathcal{X}}f \d \nu,\ f:\mathcal{X}\to[-1,1] \text{ measurable} \right\}\\
          &=2\sup_{A\in\mathscr{G}}|\mu(A) - \nu(A)|
       \end{align*}
      A sequence of measures $\{\mu_k\}_{k\in\N}$ converges in the total variation
      to a measure $\mu$ if $d_{\mathrm{TV}}(\mu_k(A)-\mu(A))\to 0$ as $k\to\infty$
      for all $A\in\mathscr{G}$.
 \item (Weak convergence). The sequence of measures $\{\mu_k\}_{k\in\N}$ is said to converge 
       in the weak sense, denoted by $\mu_k \rightharpoonup \mu$, if any of the conditions 
       of the \textit{Portmanteau Theorem} hold; these are
       \begin{enumerate}[i.]
        \item $\E_{\mu_k} f\to \E_{\mu} f$ for all bounded continuous functions $f$
        \item $\E_{\mu_k} f\to \E_{\mu} f$ for all bounded Lipschitz functions $f$
        \item $\limsup_k\E_{\mu_k} f \leq \E_{\mu} f$ for every upper semicontinuous $f$ bounded from above
        \item $\liminf_k\E_{\mu_k} f \geq \E_{\mu} f$ for every lower semicontinuous $f$ bounded from below
        \item $\limsup \mu_k(C) \leq \mu(C)$ for all closed set $C\subseteq \mathcal{X}$
        \item $\liminf \mu_k(O) \geq \mu(O)$ for all open set $O\subseteq \mathcal{X}$
       \end{enumerate}
 \item (Tightness). A sequence of measures $(\mu_n)_n$ is called \textit{tight} if for every $\epsilon>0$
       there is a compact set $K$ so that $\mu_n(K)>1-\epsilon$ for all $n\in\N$.
       
 \item (Prokhorov's Theorem). If $(\mu_n)_n$ is tight, then every subsequence of it has a further subsequence
       which is weakly convergent. 
       
 \item (L\'{e}vy-Prokhorov distance). Let $(X,d)$ be a metric space and let $\B_X$ be the Borel $\sigma$-algebra
       which makes $(X, \B_X)$ a measurable space. Let $\prob(X)$ be the space of all probability measures on 
       $(X, \B_X)$. For all $A\subseteq X$ we define
       \[
        A^\epsilon \dfn \{p \in X {}\mid{} \exists q\in A, d(p,q)<\epsilon\} = \bigcup_{p\in A} B_\epsilon(p),
       \]
       where $B_\epsilon(p)$ is an open ball centered at $p$ with radius $\epsilon$. 
       
       The L\'{e}vy-Prokhorov distance is a mapping $\pi:\prob(X)\times \prob(X)\to [0,1]$
       between two probability measures $\mu$ and $\nu$ defined as
       \[
        \pi(\mu,\nu) \dfn  \inf\{\epsilon>0 {}\mid{} \mu(A) \leq \nu(A^\epsilon)+\epsilon, \nu(A) \leq \mu(A^\epsilon)+\epsilon, \forall A\in \B_X\}.
       \]

 \item (Metrizability of weak convergence). If $(X, d)$ is a separable metric space, then convergence of a sequence of measures in the 
       L\'{e}vy-Prokhorov distance is equivalent to weak convergence.
       
 \item (Separability of $(\prob_X, \pi)$). The space $(\prob_X, \pi)$ is separable if and only if $(X,d)$ is separable.
       
 \item (Skorokhod's representation theorem). Let $(\mu_n)_n$ be a sequence of probability measures on a metric measurable space $(S,\HH)$
       such that $\mu_n\to \mu$ weakly. Suppose that the support of $\mu$ is separable\footnote{The support of a measure $\mu$
       on $\ofp$ which is equipped with a topology $\tau$ is the set of $\omega\in\Omega$ for which every open 
       neighbourhood $N_{\omega}$ of $\omega$ has a positive measure:
       $\mathrm{supp}(\mu)=\{\omega\in\Omega: \mu(N_x)>0, \text{ for all } N_\omega\in \tau, N_\omega\ni \omega\}$.}.
       Then, there exist random variables $(X_n)_n$ and $X$ on a common probability space such that the distribution 
       of $X_n$ is $\mu_n$, the distribution of $X$ is $\mu$ and $X_n\to X$ almost surely.
        
 \item (Strong $\nRightarrow$ TV).
\end{enumerate}

\subsection{Almost sure convergence}
\begin{enumerate}
 \item (Almost sure convergence). A sequence of random variables $(X_n)_n$ is said to converge \textit{almost surely}
       if the sequence $(X_n(\omega))_n$ converges (somewhere) for almost every $\omega$. It converges almost surely to $X$
       if $\lim_n X_n(\omega) = X(\omega)$ for almost every $\omega$.
              
       
 \item (Uniqueness almost surely). If $X_n \to X$ a.s. and $X_n \to Y$ a.s., then $X=Y$ a.s.
 
 \item (Characterization of a.s. convergence). The sequence $(X_n)_n$ converges a.s. to $X$ if and only if for every $\epsilon>0$
       \[
        \sum_{n\in\N}1_{(\epsilon,\infty)}\circ|X_n - X| < \infty.
       \]

 \item (Characterization of a.s. convergence \textit{a l\`a} Borel-Cantelli \#1).
       The sequence $(X_n)_n$ converges a.s. to $X$ if for every $\epsilon>0$
       \[
        \sum_{n\in\N}\prob[|X_n-X|>\epsilon] < \infty.
       \]

 \item (Characterization of a.s. convergence \textit{a l\`a} Borel-Cantelli \#2).
       The sequence $(X_n)_n$ converges a.s. to $X$ if there is a decreasing sequence $(\epsilon_n)_n$
       converging to $0$ so that 
       \[
        \sum_{n\in\N}\prob[|X_n-X|>\epsilon_n] < \infty.
       \]     
       
 \item (Cauchy criterion). The sequence $\{X_n\}_n$ is convergent almost surely if and only if
       $\lim_{m,n\to \infty}|X_n-X_m|\to 0$ almost surely.
       
 \item (Continuous mapping theorem). Let $X_n \overset{a.s.}{\to} X$ and $g$ be a (almost everywhere)
       continuous mapping. Then
       $g(X_n) \overset{a.s.}{\to} g(X)$.
       
 \item (Topological (non) characterization).
       The concept of almost sure convergence does not come from a topology on the space 
       of random variables. This means there is no topology on the space of random variables 
       such that the almost surely convergent sequences are exactly the converging sequences 
       with respect to that topology. In particular, there is no metric of almost sure convergence.        
              
\end{enumerate}

\subsection{Convergence in probability}       
\begin{enumerate}
 \item (Convergence in probability). We say that the stochastic process $(X_n)_n$ converges to a random variable $X$
       in probability if for every $\epsilon>0$,
       \[
        \lim_n \prob[|X_n-X|>\epsilon] = 0.
       \]
       We denote $X_n \overset{p}{\to} X$.
 \item (Continuous mapping theorem). Let $X_n \overset{p}{\to} X$ and $g$ be a (almost everywhere) continuous mapping. Then
       $g(X_n) \overset{p}{\to} g(X)$.
 \item (Metrizability). Convergence in probability defines a topology which is metrizable via the \textit{Ky Fan metric}
      \[
	d(X,Y) = \inf \{\epsilon>0 {}\mid{} \prob[|X-Y|>\epsilon]\leq \epsilon\} = \E[\min(|X-Y|,1)].
      \]
 \item (Metrizability \#2). 
       The sequence $X_n$ converges to $0$ in probability if and only if
       \[
        \E\left[ \frac{|X_n|}{1+|X_n|}\right] \to 0.
       \]
       The functional 
       \[
        d(X,Y) \dfn \E\left[ \frac{|X - Y|}{1+|X - Y|}\right]
       \]
       is a metric that induces the convergence in probability (provided we identify two random variables
       as equal if they are almost everywhere equal).
 \item (Almost surely convergent subsequence). 
       If $X_n \overset{p}{\to} X$, then there exists a subsequence of $(X_n)_n$, 
       $(X_{k_n})_n$ which converges almost surely to $X$.
       
 \item (Sum of independent variables). Let $(X_n)_n$ be a sequence of independent random 
       variables and let $(S_n)_n$ be a sequence defined as $S_n = X_1 + \ldots + X_n$.
       Then $S_n$ converges almost surely if and only if it converges in probability.      
 
 \item (Convergence of pairs). If $X_n\to X$ in probability and $Y_n\to Y$ in probability, then
       $(X_n, Y_n)\to (X,Y)$ in probability.
       
 \item (Almost surely $\Rightarrow$ in probability). If a sequence of random variables $\{X_k\}_k$ 
       converges almost surely, it converges in probability to the same limit.
       
 \item (In probability $\not\Rightarrow$ almost surely). 
       There are sequences which converge in probability but not almost surely. Here is an example:
              Let $(X_n)_n$ be a sequence of independent random variables on $\Omega=\N$
              with $X_n=1$ with probability $1/n$ and $0$ with probability $1-1/n$.
              Then, for any $\epsilon>0$ it is $\prob[|X_n|>\epsilon]=\tfrac{1}{n}\to 0$,
              but by the second Borel-Cantelli lemma since $\sum_{n=1}^{\infty}\prob[|X_n|>\epsilon]$
              (and the events $\{|X_n|>\epsilon\}$ are independent), we have $\prob[\limsup_n \{|X_n|>\epsilon\}]=1$.
       
\end{enumerate} 
 
\subsection{Convergence in $\mathcal{L}^p$}
\begin{enumerate}        
 \item (Convergence in $\mathcal{L}^p\ofp$). We say that $X_k$ converges to $X$ in $\mathcal{L}^p$ 
       if $X,X_k\in\mathcal{L}^p$ for all $k\in\N$ and $\|X_k-X\|_p\to 0$.
 \item (Convergence $\mathcal{L}_1$ under uniform integrability). If $X_n \to X$ in probability and $(X_n)_n$ is uniformly
       integrable, then $X_n\to X$ in $\mathcal{L}_1$.
 \item (In $\mathcal{L}_s\ofp$ $\Rightarrow$ in $\mathcal{L}^p\ofp$, for $s>p\geq 1$). 
 \item (Scheff\'e's theorem). Let $X_n\in\mathcal{L}_1$, $X\in\mathcal{L}_1$ and $X_n\to X$ almost surely. The following are 
       equivalent:
       \begin{enumerate}[i.]
        \item $\E[|X_n|] \to \E[|X|]$,
        \item $\E[|X_n - X|]\to  0$.
       \end{enumerate} 
 \item (Convergence in $\mathcal{L}^p$ for all $p\in[1,\infty)$ but not in $\mathcal{L}_\infty$).
       Let $X$ be a random variable on $\Omega=\N$ which follows the Poisson distribution ($\prob[X=k]=\frac{e^{-\lambda}\lambda^k}{k!}$, $\lambda>0$).
       Define the sequence $X_k = 1_{\{X=k\}}$. Then $\|X_k\|_\infty=1$.
 \item (Vitali's theorem).
       Suppose that $X_n\in\mathcal{L}^p$, $p\in[1,\infty)$ and $X_n\to X$ in probability.
       The following are equivalent
       \begin{enumerate}[i.]
        \item $\{X_n\}_n$ is uniformly integrable
        \item $X_n\to X$ in $\mathcal{L}^p$
        \item $\E[|X_n|^p] \to \E[|X|^p]$
       \end{enumerate}

 \item (In $\mathcal{L}^p$ $\Rightarrow$ in probability). If $(X_n)_n$ converges to $X$ in $\mathcal{L}^p$, for 
       any $p\in [1,\infty]$ it also converges to $X$ in probability. 
       
 \item (Almost surely $\not\Rightarrow$ in $\mathcal{L}^p$). On $([0,1], \mathcal{B}_{[0,1]}, \lambda)$ take $X_n=n 1_{[0,1/n]}$.
       Then, for all $p\in[1,\infty]$ we have $\|X_n\|_p=1$, but the sequence converges almost surely to $0$.
       
 \item (In $\mathcal{L}^p$, $p\in[1,2)$ $\not\Rightarrow$ In $\mathcal{L}^p$, for $p\geq 2$).
       Let $\Omega=\N$ and $Z_k^p$ be a sequence of random variables with parameter $p$ and 
       \begin{align*}
        \prob[Z_k^p=n] &= pn,\\
        \prob[Z_k^p=0] &= 1-pn.
       \end{align*}
       Let $X=0$ and $X_k$ be defined as
       \[
        X_k = Z_{k}^{p_k}
       \]
       where $p_k=\nicefrac{1}{k^2 \ln k}$. 
       Then $\E[|X_k|^t]=\nicefrac{k^{t-2}}{\ln k}$. 
       We have $\E[|X_k|^t]\to 0$ if and only if $t<2$.
\end{enumerate}
\begin{figure}
 \centering
 \includegraphics[width=0.6\linewidth]{figures/convergence_rv}
 \caption{Illustration of the relationships among different modes of convergence of random variables. 
         Convergence in $\mathcal{L}_\infty$ implies convergence in $\mathcal{L}^p$ for all $p\in[1,\infty)$
         which in turn implies convergence in $\mathcal{L}_{p'}$ for all $1\leq p' \leq p$ which implies 
         convergence in probability which implies convergence in distribution which implies 
         convergence of the characteristic functions (L\'evy's continuity theorem). 
         Convergence in distribution implies almost 
         convergence of a sequence of RVs $\{Y_k\}_k$ which have the same distribution as 
         $\{X_k\}_k$ ($Y_k \overset{d}{\sim} X_k$ and $Y \overset{d}{\sim} X$).
         }
\end{figure}


\subsection{Convergence in distribution}
\begin{enumerate}
\item (Convergence in distribution). The sequence of random variables $\{X_n\}_n$ with distributions 
      $\{\mu_n\}_n$ is said to converge in distribution of $X$ if $\{\mu_n\}_n$ converges weakly
      to $\mu$, the distribution of $X$.
      
\item (Slutsky's theorem). Let $X_k\to X$ in distribution and $Y_n\to c$ in probability, where $c$ is a constant.
      Then, 
      \begin{enumerate}[i.]
       \item $X_n + Y_n \to X + c$ in distribution
       \item $X_nY_n \to cX$ in distribution
       \item $X_n/Y_n \to X/c$ in distribution, provided that $c\neq 0$, $Y_n\neq 0$.
      \end{enumerate}
      
\item (Almost sure convergence). If $X_n\to X$ in distribution, we may find a probability space $\ofp$
      and random variables $Y$ and $(Y_n)_n$ so that $Y_n$ is equal in distribution to $X_n$,
      $Y$ is equal in distribution to $X$ and $Y_n\to Y$ almost surely.
      
\item (L{\'e}vy's continuity theorem)%
	  \footnote{Lecture notes 6.436J/15.085J by MIT, Available online at \url{https://goo.gl/7ZaHW9}.}. 
      Let $\{X_k\}_k$ be a sequence of random variables with characteristic functions $\varphi_k(t)$ and let $X$ be a random variable with characteristic function $\varphi(t)$. It $X_k$ converges to $X$ in distribution then $\varphi_k \to \varphi$ pointwise. Conversely, if $\varphi_k \to \varphi$ and $\varphi$ is continuous at $0$, then $\varphi$ is the characteristic function of a random variable $X$ and $X_k\to X$ in distribution.
      the 
\item (Scheff\'e's theorem for density functions).\footnote{S. Sagitov, ``Weak Convergence of Probability Measures,'' 2013, Available at: \url{https://goo.gl/m4Qi5i}}
Let $\prob_n$ and $\prob$ have densities $f_n$ and $f$
      with respect to a measure $\mu$. If $f_n \to f$ $\mu$-a.s., then $\prob_n \to \prob$ in the total 
      variation metric and, as a result,  $\prob_n \to \prob$ weakly.
      
\item (Continuous mapping theorem). For a (almost everywhere) continuous function $g$, if the sequence $\{X_k\}_k$ 
      converges in distribution to $X$, then $\{g(X_k)\}_k$ converges in distribution to 
      $g(X)$.
      
\item (Convergence in probability $\Rightarrow$ in distribution). If $\{X_k\}_k$ converges in probability,
      then it converges in distribution to the same limit.
      
\item (In distribution $\not\Rightarrow$ in probability). There are sequences which converge in distribution,
      but not in probability. For example: On the space $([0,1], \mathcal{B}_{[0,1]},\lambda)$, let $X_{2n}(\omega)=\omega$
      and $X_{2n-1}(\omega) = 1-\omega$. Then all $X_k$ have the same distribution, but the sequence does not 
      converge in probability. As a second example, the sequence $X_n=X$ where $X$ follows the Bernoulli distribution
      with parameter $\frac{1}{2}$, converges in distribution to $1-X$, but not in probability.
      
\item (Polya-Cantelli lemma)\footnote{Lecture notes of M. Banerjee, Available at: \url{http://dept.stat.lsa.umich.edu/~moulib/ch2.pdf}}. If $X_n\to X$ in distribution, $F_n$ are the distribution function of $X_n$ and 
      $X$ has the \textit{continuous} distribution function $F$, then $\|F_n - F\|_\infty \dfn \sup_{x}|F_n(x) - F(x)| \to 0$
      as $n\to\infty$.
\end{enumerate}



\subsection{Tail events and 0-1 Laws}
\begin{enumerate}

 \item (Simple 0-1 law). Let $\{E_n\}$ be a sequence of independent events. Then $\prob[\limsup_n E_n]\in \{0,1\}$.
 \item (Unions of $\sigma$-algebras). Let $\F_1$, $\F_2$ be two $\sigma$-algebras on a nonempty set $X$.
       The $\sigma$-algebra generated by the sets $E_1\cup E_2$ with $E_1\in\F_1$ and $E_2\in\F_2$ is 
       denoted by $\F_1 \vee \F_2$
 
 \item (Tail $\sigma$-algebra). Let $(\F_n)_{n}$ be a sequence of sub-$\sigma$-algebras of $\F$.
       The $\sigma$-algebra $T_n\dfn \bigvee_{m>n}\F_m$ encodes the information about the future 
       after $n$ and $T=\bigcap_n T_n$ is the \textit{tail $\sigma$-algebra} which encodes the 
       information of the end of time. 
       
 \item (Events in the tail $\sigma$-algebra). For a process $(E_n)_n$ be a sequence of events. 
       The associated tail $\sigma$-algebra $T$ is $\bigcap_n \sigma(\{E_k\}_{k\geq n})$.
       The event $\limsup_n E_n$ is in $T$.
 
 \item (Kolmogorov's zero-one law). Let $(\F_n)_n$ be a sequence of \textit{independent}
       $\sigma$-algebras on a nonempty set $X$ and let $T$ be the tail $\sigma$-algebra.
       We equip $(X,\F)$ with a probability measure $\prob$. For every $H\in T$,
       $\prob(H)\in\{0,1\}$.
 
 \item (Counterpart of the Borel-Cantelli lemma). 
 Let $\{E_n\}_{n\in\N}$ be a nested increasing sequence of events in $\ofp$, that is 
 $E_k\subseteq E_{k+1}$ and let $E_k^c$ denote the complement of $E_k$.
 Infinitely many $E_k$ occur with probability $1$ if and only if there is an increasing sequence 
 $t_k\in\N$ such that
 \[
  \sum_k \prob[A_{t_{k+1}}\mid A_{t_k}^c]  = \infty.
 \] 
 
 \item (L\'evy's zero-one law). Let $\Ff=\{\F_k\}_{k\in\N}$ be any filtration of $\F$ on $\ofp$ and
 $X\in\mathcal{L}_1\ofp$. Let $\F_\infty$ be the minimum $\sigma$-algebra generated by $\Ff$. Then
 \[
  \E[X\mid \F_k] \to \E[X\mid \F_\infty], 
 \]
 both in $\mathcal{L}_1\ofp$ and $\prob$-a.s.
\end{enumerate}



\subsection{Laws of large numbers and CLTs} 
\begin{enumerate}
  \item (Weak law of large numbers). Also known as Bernoulli's theorem. Let $\{X_k\}_k$ be
       a sequence of independent identically distributed random variables, each having a 
       finite mean $\E[X_k]=\mu$ and finite variance $\sigma^2$. Define 
       $\bar{X}_k=\nicefrac{1}{k}(X_1+\ldots+X_k)$. Then $\bar{X}_k\to \mu$ in probability.
   \item (Strong law of large numbers). Let $\{X_k\}_k$ and $\bar{X}_k$ be as above.
         Then $\bar{X}_k\to \mu$ almost surely.    
   \item (Uniform law of large numbers).  Let $f(x,\theta)$ be a function defined over
         $\theta\in\Theta$. For fixed $\theta$ and 
         a random process $\{X_k\}_k$ define $Z_k^\theta \dfn f(X_k, \theta)$. Let 
         $\{Z_k^\theta\}_k$ be a sequence of independent and identically distributed 
         random variables, such that the sample mean converges in probability to $\E[f(X, \theta)]$. 
         Suppose that (i) $\Theta$ is compact, (ii) $f$ is continuous 
         in $\theta$ for almost all $x$ and measurable with respect to $x$ for each $\theta$,
         (iii) there is a function $g$ such that $\E[g(X)]<\infty$ and $\|f(x,\theta)\| \leq g(x)$ 
         for all $\theta\in\Theta$. Then, $\E[f(X,\theta)]$ is continuous in $\theta$ and
         \[
          \sup_{\theta\in\Theta}\left\| 
	    \bar{Z}^\theta_k - \E[f(X,\theta)] 
          \right\|
          \overset{a.s.}{\longrightarrow} 0
         \]

   \item (Lindeberg-L{\'e}vy central limit theorem). Let $\{X_k\}_k$ be iid, finite mean and variance 
         and $\bar{X}_k$ as above. Then
         \[
          \frac{\bar{X}_k-\mu}{\sqrt{n}} \overset{d}{\longrightarrow} N(0, \sigma^2).
         \]

   \item (Lyapunov central limit theorem). Let $\{X_k\}_k$ be a sequence of independent random variables
         with $\E[X_k]=\mu_k$ and finite variance $\sigma_k^2$. Define $s_k^2 = \sum_{i=1}^{k}\sigma_i^2$.
         If for some $\delta>0$, the following condition holds (Lyapunov's condition)%
         \footnote{In practice it is usually easiest to check Lyapunov's condition for $\delta=1$. 
                   If a sequence of random variables satisfies Lyapunov's condition, then it also 
                   satisfies Lindeberg's condition. The converse implication, however, does not hold.}:
         \[
          \lim_{k\to\infty} \frac{1}{s_k^{2+\delta}}\sum_{i=1}^{k}\E\left[ 
          |X_i - \mu_i|^{2+\delta}
          \right]=0,
         \]
	 then,
	 \[
	  \frac{1}{s_k}\sum_{i=1}^{k}(X_i - \mu_i) \overset{d}{\longrightarrow} N(0, 1).
	 \]

\end{enumerate}



\section{Stochastic Processes}
\subsection{General}
\begin{enumerate}
 \item (Stochastic process). Let $\T \subseteq \barre$ (e.g., $T=\N$ or $T=\barre$). A random process is 
       a sequence/net $(X_n)_{n\in \T}$ of random variables on a probability space $\ofp$.
       
 \item (Filtrations). A filtration is an increasing sequence of sub-$\sigma$-algebras of $\F$. The space 
       $(\Omega, \F, (\F_t)_{t\in \T}, \prob)$ is called a filtered probability space. The filtration 
       $\F_t = \sigma(\{X_s; s\in \T, s\leq t\})$ is called the filtration \textit{generated by $(X_n)_{n\in \T}$}.
       We say that $(X_n)_n$ is adapted to a filtration $(\F_n)_n$ if for all $n\in \T$, $X_n$ is $\F_n$-measurable.
       
 \item (Stopping times). Let $(\F_n)_n$ be a filtration on $\ofp$ and define $\overline{\T} \dfn \T \cup \{+\infty\}$.
       A random variable $T: \Omega \to \overline{\T}$ is called a stopping time if
       \[
        \{\omega {}\mid{} T(\omega) \leq t\} \in \F_t,
       \]
      for all $t\in \T$. This is equivalent to requiring that the process $Z_t = 1_{T \leq t}$ is adapted to $(\F_t)_{t\in \T}$.
      
 \item (A useful property). For any stochastic process $(X_n)_{n\in\N}$, we have
 \[
  \prob\left(\max_{i\leq k}|X_i|>\epsilon\right) = \prob\left(\sum_{i=0}^{k}X_i^2 \cdot  1_{\{|X_i|>\epsilon\}}>\epsilon^2\right).
 \]

\end{enumerate}



\subsection{Martingales}

\begin{enumerate}
 \item (Martingale). A random process $(X_n)_n$ is called a \textit{martingale} if 
		     $\E[|X_n|]<\infty$ and $\E[X_{n+1}{}\mid{}X_1,\ldots, X_n]=X_{n}$.
 \item (Martingale examples). The following are common examples of martingales:
  \begin{enumerate}
   \item Let $(X_n)_n$ be a sequence of iid random variables with mean $\E[X_n]=\mu$. Then
         $Y_n = \sum_{i=1}^{n}(X_i-\mu)$ is a martingale.
   \item If $(X_n)_n$ is a sequence of iid random variables with mean $1$, then 
         $Y_n = \prod_{i=1}^{n}X_i$ is a martingale.
   \item If $(X_n)_n$ is a sequence of random variables with finite expectation and 
         $\E[X_n{}\mid{}X_1,\ldots,X_{n-1}]=0$, then $Y_n=\sum_{i=0}^{n}X_i$ is a 
         martingale.
   \item (The classical martingale). The fortune of a gambler is a martingale in a fair game.
  \end{enumerate}
 \item (Sub- and super-martingales). A random process $(X_n)_n$ is called a \textit{supermartingale} if 
		     $\E[|X_n|]<\infty$ and $\E[X_{n+1}{}\mid{}X_1,\ldots, X_n]\leq X_{n}$. Likewise, it 
		     is a \textit{submartingale} if $\E[|X_n|]<\infty$ and $\E[X_{n+1}{}\mid{}X_1,\ldots, X_n]\geq X_{n}$.
 \item (Stopping time). Let $\{Z_k\}_k$ be a random process and $T$ a stopping time. 
       Define $X_k(\omega) = Z_{k\wedge T(\omega)}$, that is
       \[
        X_k(\omega) = \begin{cases}
                       Z_k(\omega),&\text{if }k \leq T(\omega)\\
                       Z_{T(\omega)}(\omega),&\text{otherwise}
                      \end{cases}
       \]
       If $Z$ is a (sub)matringale, then $X$ is a (sub)matringale too.
 \item (Martingale stopping).
 \item (Almost sure martingale convergence). Let $(X_n)_n$ be a martingale which is uniformly 
       bounded in $\mathcal{L}_1$, i.e., $\sup_n \E[|X_n|] < \infty$. Then, there is a $X\in\mathcal{L}_1(\mathcal{F}_\infty)$,
       so that $X_n\to X$ a.s., where $\mathcal{F}_\infty = \sigma(\mathcal{F}_n, n\geq 0)$.
 \item (Kolmogorov's submartingale inequality). Let $\{X_k\}_k$ be a nonnegative submartingale.
       Then, for $n\in\N_{>0}$ and $\alpha>0$,
       \[
        \prob\left[ \max_{k=1,\ldots, n}X_k \geq \alpha\right] \leq \frac{\E[X_n]}{\alpha}.
       \]
       \begin{enumerate}[i.]
        \item (Corollary 1). Let $\{X_k\}_k$ be a nonnegative martingale. 
              Then $\prob[\sup_{k\geq 1}X_k \leq \alpha]\leq\E[X_1]/\alpha$ for $\alpha>0$.
        \item (Corollary 2). Let $\{X_k\}_k$ be a martingale with $\E[X_k^2]< \infty$ for all $k\in\N_{>0}$. 
               Then, $\prob[\max_{k=1,\ldots, n}|X_k|\geq \alpha]\leq \E[X_n^2]/\alpha$ for all $n\in \N_{\geq 2}$
               and $\alpha>0$.
        \item (Corollary 3). Let $\{X_k\}_k$ be a nonnegative supermartingale. Then, for $n\in\N_{>0}$
              and $\alpha>0$, $\prob[\cup_{k\geq n}\{Z_k\geq \alpha\}]\leq\E[Z_n]/\alpha$.
       \end{enumerate}


 \item (Azuma-Hoeffding inequality for martingales with bounded differences). Let $(X_i)_i$
       be a martingale or a  supermartingale and $|X_k-X_{k-1}|<c_{k}$ almost surely. Then for all $N\in\N$
       and $t\in\Re$,
       \[
        \prob[X_N-X_0\geq t] \leq \exp \left(-\frac{t^2}{2\sum_{i=1}^{N}c_i^2}\right)
       \]
       If $(X_i)_i$ is a submartingale, 
       \[
        \prob[X_N-X_0\leq -t] \leq \exp \left(-\frac{t^2}{2\sum_{i=1}^{N}c_i^2}\right)
       \]

\end{enumerate}


\subsection{Markov processes}
\begin{enumerate}
 \item (Definition). Let $(\Omega, \F, \{\F_t\}_{t\in T}, \prob)$ be a filtered probability space.
       Let $\{X_t\}_{t\in T}$ be a random process which is adapted to the filtration $\{\F_t\}_{t\in T}$.
       Let $\{\mathcal{G}^0_t\}_{t\in T}$ be the filtration generated by $\{X_t\}_{t\in T}$ and 
       $\mathcal{G}^\infty_t = \sigma(\{X_u: u\geq t, u\in T\})$. The process is said to be
       Markovian if for every $t\in T$, the past $\F_t$ and the future $\G^\infty_t$ are conditionally
       independent given $X_t$.
 \item (Characterization). The following are equivalent
       \begin{enumerate}[i.]
        \item The process $\{X_t\}_t$ is Markovian with state space $(E, \mathcal{E})$
        \item For every $t\in T$ and $u>t$, and $f\in\mathcal{E}_+$
           $
             \E[f\circ X_u\mid \F_t] = \E[f\circ X_u\mid X_t].
           $
        \item Let $E$ be a p-system generating $\mathcal{E}$.
              For every $t\in T$ and $u>t$, and $A\in E$ it is
              $
                \E[1_A\circ X_u\mid \F_t] = \E[1_A\circ X_u\mid X_t].
              $
        \item For every $t\in T$ and positive $V\in \mathcal{G}^\infty_t$,
           $
             \E[V\mid \F_t] = E[V\mid X_t].
           $
        \item For every $t\in T$ and positive $V\in \mathcal{G}^\infty_t$,
           $
             \E[V\mid \F_t] \in \sigma X_t.
           $
       \end{enumerate}

\end{enumerate}

\subsection{Markov decision processes}

\subsubsection{General}
\begin{enumerate}
 \item (Definition of MCM). 
       A Markov control model (MCM) is a tuple
       \(\left(\mathcal{X},\mathcal{U}, U, Q, c\right)\) consisting of 
       \begin{enumerate}[i.]
        \item two Borel spaces $\mathcal{X}$ and $\mathcal{U}$ called the state and control spaces respectively,
        
        \item a multivalued function mapping $U:\mathcal{X} \rightrightarrows \mathcal{U}$ which maps
              a state $x\in\mathcal{X}$ to a set $U(x)\subseteq \mathcal{U}$ of feasible control actions
              Define the graph of $U$ as the set
	      $\mathcal{K} \dfn \operatorname{gph}(U)=\{(x,u)\in \mathcal{X}\times \mathcal{U} \mid u\in U(x)\}$.
	      We assume that $\mathcal{K}$ contains the graph of a measurable (single-valued) function $u:\mathcal{X}\to\mathcal{U}$.
	      
        \item a transition kernel 
              $Q:\mathcal{K}\times \mathcal{X}\ni(x,u,B)\mapsto Q(x,u;B)\in [0,1]$, where $(x,u)$ 
              is such that $u\in U(x)$ and $B\in\mathcal{B}(\mathcal{X})$.              
              
	\item A cost function $c$ is a measurable function $c:\mathcal{K}\to \Re$.
       \end{enumerate}
       
 \item (Definition of $\mathcal{H}_t$ and $\overline{\mathcal{H}}_t$). Define $\mathcal{H}_0=\mathcal{X}$ and $\mathcal{H}_t = \mathcal{K}^t\times \mathcal{X}$.
       $\mathcal{H}_t$ contains elements of the form $h_t = (x_0,u_0,\ldots,x_{t-1}, u_{t-1}, x_t)$ with $u_k\in U(x_k)$.
       Define also the linear space $\overline{\mathcal{H}}_t = (\mathcal{X}\times \mathcal{U})^{t}\times \mathcal{X}$ with $\overline{\mathcal{H}}_0=\mathcal{X}$.
 \item (Definition of a policy). A policy is a sequence $\pi=(\pi_0,\pi_1,\ldots)$ of transition kernels $\pi_t:\mathcal{B}(\mathcal{U})\times \mathcal{H}_t\to [0,1]$ with 
	\[
	  \pi_t(A(x_t), h_t) = 1,\text{ for all } h_t\in H_t, t\in\N.
	\]
 \item (The canonical probability space $\ofp$). Given an MCM \(\left(\mathcal{X},\mathcal{U}, U, Q, c\right)\), let $\Omega=\overline{\mathcal{H}}_{\infty} = \prod_{t=1}^{\infty}\mathcal{X}\times\mathcal{U}$. 
       $\Omega$ contains sequences $\omega = (x_0,u_0,x_1,u_1,\ldots)$.
       Let $\F$ be the corresponding product $\sigma$-algebra. Given a probability measure $\nu$ on $(\mathcal{X}, \mathcal{B}(\mathcal{X}))$ (called the initial
       distribution) and a policy $\pi$, according to the Ionescu-Tulcea Theorem, there is a unique probability measure $\prob_\nu^\pi$ so that
       for $B\in\mathcal{B}(\mathcal{X})$, $C\in\mathcal{B}(\mathcal{U})$, $h_t\in\mathcal{H}_t$ and $t\in\N$:
       \begin{enumerate}[i.]
        \item $\prob_\nu^\pi[x_0\in B] = \nu(B)$
        \item $\prob_\nu^\pi[u_t\in C\mid h_t] = \pi_t(C, h_t)$
        \item $\prob_\nu^\pi[x_{t+1}\in B\mid h_t, u_t] = Q(B, x_t, u_t)$
       \end{enumerate}

 \item (Markov decision process). A (discrete-time) Markov decision process (MDP) is a tuple $(\Omega, \F, \prob_{\nu}^{\pi}, \{x_t\}_t)$. In other words, 
       for a given policy $\pi$ and a given initial distribution $\nu$, an MDP is a stochastic process $\{x_t(\omega)\}_{t\in\N}$ over the 
       canonical probability space $(\Omega, \F, \prob_{\nu}^{\pi})$.
       
 \item (Types of policies).       
       
\end{enumerate}

\subsubsection{Optimal control problems}


\section{Information Theory}
\subsection{Entropy and Conditional Entropy}
\begin{enumerate}
 \item (Self-Information, construction). Let $\ofp$ be a discrete probability space. A self-information function $I$ must satisfy the 
       following desiderata: 
       (i) if $\omega_i$ is sure ($\prob[\omega_i]=1$), then this offers no information, that is $I(\omega_i)=0$, 
       (ii) if $\omega_i$ is not sure, that is $\prob[\omega_i]<1$, then $I(\omega_i)>0$,
       (iii) $I(\omega)$ depends on the probability $\prob[\omega]$, that is, there is a function $f$ so that $I(\omega)=f(\prob[\omega])$
       (iv) for two independent events $A$ and $B$, $I(A\cap B)=I(A)+I(B)$.
 \item (Self-information, definition). A definition which satisfied the above desiderata is $I(\omega)=-\log(\prob[\omega])$. 
 \item (Self-information, units). When $\log_2$ is used in the definition, the units of measurement of self-information are the \textit{bits}.
       If $\ln\equiv \log_e$ is used, the self-information is measures in \textit{nats}. 
       For the decimal logarithm, $I$ is measured in \textit{hartley}.
 \item (Entropy, definition). The \textit{entropy} (or Shannon entropy) of a random variable is the expectation of its self-information denoted 
       as $H(X)=\E[I(X)]$, where $I(X)$ is to be interpreted as follows: Let $\ofp$ be a probability space and $X:\ofp\to \{x_i\}_{i=1}^{n}$
       a finite-valued random variable. Consider the events $E_i=\{\omega\in\Omega {}\mid{} X(\omega) = x_i\}$ with self-information 
       $I(E_i)$. Then, $I(X)$ is the random variable $I(X)(\omega) = I(E_{\iota(\omega)})$, where $\iota(\omega)$ is such 
       that $X(\omega) = x_{\iota(\omega)}$.
       
       The entropy of $X$ is given by
       \[
        H(X) = -\sum_{i=1}^{n}p_i \log(p_i),
       \]
       where $p_i = \prob[X=x_i]$.
 \item (Joint entropy). The \textit{joint entropy} of two random variables $X$ and $Y$ (with values $\{x_i\}_i$ and $\{y_j\}_j$
       respectively) is the entropy of the random variable 
       $(X,Y)$ in the product space, that is
       \[
        H(X,Y) = -\sum_{i,j}p_{ij}\log p_{ij},
       \]
       where $p_{ij} = \prob[X=x_i, Y=y_j]$.
 \item (Conditional Entropy).
 \item (Mutual information).
\end{enumerate}

\subsection{KL divergence}
\begin{enumerate}
 \item (Definition/Discrete spaces). Let $(\Omega, \F)$ be a discrete measurable space and $\prob$ and $\prob'$ two probability measures
       on it. The Kullback-Leibler (KL) divergence from $\prob'$ and $\prob$ is defined as%
	  \footnote{Lecture notes by S. Khudanpur available online at \url{https://www.clsp.jhu.edu/~sanjeev/520.447/Spring00/I-divergence-properties.ps}}
       \[
        \mathrm{D}_{\mathrm{KL}}(\prob \| \prob') = -\sum_i \prob_i \log \left(\nicefrac{\prob'_i}{\prob_i}\right) = \sum_i \prob_i \log \left(\nicefrac{\prob_i}{\prob'_i}\right)
       \]
 \item (Definition/Continuous spaces with PDFs). The KL divergence over a continuous probability space and for two 
       probability measures $\prob$ and $\prob'$  with PDFs $p$ and $p'$ respectively is
       \[
        \mathrm{D}_{\mathrm{KL}}(\prob \| \prob') = \int_{-\infty}^{\infty}p(x)\log\left(\nicefrac{p(x)}{p'(x)}\right)\d x
       \]       
 \item (Definition/Continuous spaces). If $\prob$ is absolutely continuous with respect to $\prob'$, we define
       \[
        \mathrm{D}_{\mathrm{KL}}(\prob \| \prob') = \int_{\Omega}\log \left( \nicefrac{\d\prob}{\d\prob'}\right) \d\prob.
       \]

 \item (Nonnegative). The KL divergence is always nonnegative: $\mathrm{D}_{\mathrm{KL}}(\prob \| \prob')\geq 0$
 \item (Pinsker's inequality). $d_{\mathrm{TV}}(\prob,\prob')\leq \sqrt{\tfrac{1}{2}\mathrm{D}_{\mathrm{KL}}(\prob\| \prob')}$
\end{enumerate}


\section{Theory of Risk}

\subsection{Risk measures}
\begin{enumerate}
 \item (Risk measures and coherency). Let $\ofp$ be a probability space and 
       $\mathcal{Z}=\mathcal{L}^p\ofp$ for $p\in[1,\infty]$. A risk measure 
       $\rho:\mathcal{Z}\to\barre$ is called coherent if
       \begin{enumerate}[i.]
        \item (Convexity). For $Z,Z'\in\mathcal{Z}$ and $\lambda\in[0,1]$, $\rho(\lambda Z + (1-\lambda)Z') 
              \leq \lambda \rho(Z) + (1+\lambda) \rho(Z')$
        \item (Monotonicity). For $Z,Z'\in\mathcal{Z}$, $\rho(Z)\leq \rho(Z')$ whenever $Z\leq Z'$ a.s.,
        \item (Translation equivariance). For $Z\in\mathcal{Z}$ and $C\in \mathcal{Z}$ with $C(\omega)=c$
              for almost all $\omega$ (almost surely constant), it is $\rho(C+Z) = c + \rho(Z)$,
        \item (Positive homogeneity). For $Z\in\mathcal{Z}$ and $\alpha \geq 0$, $\rho(\alpha Z) = \alpha \rho(Z)$.
       \end{enumerate}   
 \item (Conjugate risk measure). With every convex risk measure, we associate the conjugate risk measure 
       $\rho^*:\mathcal{Z}^*\to\barre$ defined as
       \[
        \rho^*(Y) = \sup_{Z\in\mathcal{Z}} \left\{\<Z,Y\> - \rho(Z) \right\}.
       \]
 \item (Biconjugate risk measure). With every convex risk measure,  we associate the biconjugate risk measure        
       $\rho^{**}:\mathcal{Z}^{**}\to\barre$
       \[
        \rho^{**}(Z) = \sup_{Y\in\mathcal{Z}^*} \left\{\<Z,Y\> - \rho^{*}(Y) \right\}.
       \]
 \item (Dual representation). Let $\mathcal{Z}=\mathcal{L}^p\ofp$ with $p\in[1,\infty)$.
       If $\rho$ is lower semicontinuous, then $\rho = \rho^{**}$. In particular,
       \[
        \rho(Z) = \sup_{Y\in\mathcal{Z}^*} \left\{\<Z,Y\> - \rho^{*}(Y) \right\} 
                = \sup_{Y\in\mathfrak{A}} \left\{\<Z,Y\> - \rho^{*}(Y) \right\},
       \]
       where $\mathfrak{A} = \dom \rho^*$.
       
 \item (Acceptance set). The set $\mathcal{A}_\rho = \{X\in\mathcal{Z} : \rho(X)\leq 0\}$ is called the acceptance 
       set of $\rho$. Several properties of $\rho$ can be tested using its acceptance set.
       
 \item (Monotonicity condition). If $Y\geq 0$ (almost surely) for every $Y\in \mathfrak{A}$, 
       then and only then $\rho$ is monotone.
       
 \item (Translation equivariance condition). If for every $Y\in\mathfrak{A}$ it is $\E[Y]=1$,
       then and only then, $\rho$ is translation equivariant.
       
 \item (Positive homogeneity condition). If $\rho$ is the support function of $\mathfrak{A}$, 
       that is, $\rho(Z) = \sup_{Y\in\mathfrak{A}}\<Y,Z\>$, then and only then it is positively homogeneous.
       $\mathfrak{A}$ is called the admissibility set of $\rho$.
       
 \item (Coherency-preserving operations). Let $\rho_1,\rho_2$ be two risk measures on $\mathcal{Z}$.
       Then, the following risk measures are coherent
       \begin{enumerate}[i.]
        \item $\rho(X) \dfn \lambda_1\rho_1(X)+\lambda_2\rho_2(X)$, $\lambda_1,\lambda_2\in\Re$ not both equal to $0$
        \item $\rho(X) = \max\{\rho_1(X), \rho_2(X)\}$
       \end{enumerate}       
 \item (Subdifferentiability). If $\rho:\mathcal{Z}\to\Re$ is real valued, convex and monotone, 
       then it is continuous and subdifferentiable on $\mathcal{Z}$.
 \item (Subdifferentials of risk measures). Let $\rho:\mathcal{L}^p\ofp \to \barre$, $p\in [1,\infty)$,
        be convex and lower semicontinuous. Then $\partial \rho(Z) = \arg\max_{Y\in\mathfrak{A}} \{\<Y,Z\> - \rho^*(Z)\}$.
        If, additionally, $\rho$ is positively homogeneous, then $\partial \rho(Z) = \arg\max_{Y\in\mathfrak{A}}\<Y,Z\>$.
 \item (Convexity of $\rho\circ F$). Let $F:\Re^n\to\mathcal{Z}$ be a convex mapping%
       \footnote{The mapping $F:\Re^n\to\mathcal{Z}$ if for every $\lambda\in[0,1]$ and $x,y\in\Re^n$
                 it is $F(\lambda x + (1-\lambda)y)(\omega) \leq \lambda F(x)(\omega) + (1-\lambda)F(y)(\omega)$
                 for $\prob$-almost every $\omega$.} and $\rho$ be a convex
       monotone risk measure. Then $\rho\circ F$ is convex. 
 \item (Directional differentiability). Let $\mathcal{Z}=\mathcal{L}^p\ofp$ with $p\in[1,\infty)$,
       $F:\Re^n\to\mathcal{Z}$ be a convex mapping and $\rho:\mathcal{Z}\to\barre$ be a convex monotone 
       risk measure which is finite-valued and continuous at $\bar{Z} = F(\bar{x})$.
       Then, $\phi\dfn \rho\circ F$ is directionally differentiable at $\bar{x}$,
       $\phi'(\bar{x};h)$ is finite-valued for all $h\in\Re^n$ and\footnote{%
       $F$ maps a vector $x$ to random variables, so it is $F(x)(\omega)=f(x,\omega)$. The directional derivative of $f$ with 
       respect to $x$ along a direction $h$ is $f'(\bar{x}; h)$ and it is a random variable. The scalar product here is defined as
       $\<Y, f'(\bar{x}; h)\> = \int_{\Omega} Y(\omega) f'(\bar{x}; h)(\omega) \d \prob(\omega)$.}
       \[
        \phi'(\bar{x}; h) = \sup_{Y \in \partial \rho(\bar{Z})}\<Y, f'(\bar{x}; h)\>
       \]
       
 \item (Subdifferentiability of $\rho\circ F$). Let $\mathcal{Z}$, $F$, $\rho$, $\bar{x}$ and $\bar{Z}$ be as above.
       Define $\phi = \rho\circ F$. 
       Then $\phi$ is subdifferentiable at $\bar{x}$ and 
       \[
        \partial \phi(\bar{x}) = \operatorname{cl} 
        \bigcup_{\substack{Y\in \partial \rho(\bar{Z})\\F'\in \partial F(\bar{x})}}\<Y, F'\>
       \]
       
 \item (Continuity equivalences). Let $\rho:\mathcal{Z}\to\barre$ be a convex, monotone, translation
       equivariant risk measure and $\mathcal{Z}=\mathcal{L}^p\ofp$. 
       The following are equivalent\footnote{%
       For a detailed discussion on continuity properties of risk measures, see D. Filipovi{\'c} 
       and G. Svindland, ``Convex risk measures on $\mathcal{L}^p$,''
       Available online at: \url{http://www.math.lmu.de/~filipo/PAPERS/crmlp.pdf}.}:
       \begin{enumerate}[i.]
        \item $\rho$ is continuous
        \item $\rho$ is continuous at a $X\in\dom \rho$
        \item $\operatorname{int} \mathcal{A}_\rho \neq \varnothing$
        \item $\rho$ is lower semi-continuous and finite-valued ($\dom \rho = \mathcal{Z}$)
       \end{enumerate}

 
 \item (Lipschitz continuity wrt infinity norm). Let $\rho:\mathcal{Z}\to\barre$ be a proper, 
       convex, monotone, translation equivariant risk measure. Then, for all $X,X'\in\dom\rho$
       \[
        \left|\rho(X)-\rho(X')\right| \leq \|X-X'\|_{\infty}.
       \]


 \item (Law invariance). A risk measure $\rho$ is called law invariant if $\rho(Z) = \rho(Z')$ whenever 
       $Z$ and $Z'$ have the same distribution.
       
 \item (Fatou property \#1). Let $\rho:\mathcal{L}^\infty\to \barre$ be a proper convex risk measure. 
       The following are equivalent:
       \begin{enumerate}[i.]
        \item $\rho$ is $\sigma(\mathcal{L}^\infty, \mathcal{L}^1)$-lower semi-continuous
        \item $\rho$ has the Fatou property, i.e., $\rho(X) \leq \liminf_{k}\rho(X_k)$ whenever
              $\{X_k\}$ is essentially uniformly bounded (there is $Z\in\mathcal{L}^\infty$ so that $X_k\leq Z$ for all $k\in\N$)
              and $X_k\overset{p}{\longrightarrow}X$.
       \end{enumerate}

 \item (Law-invariant risk measures have the Fatou property)%
	  \footnote{This result is rather involved. For a detailed
	  presentation refer to the article E. Jouini, W. Schachermayer and N. Touzi, 
	  ``Law invariant risk measures have 
	  the Fatou property,'' (Chapter) Advances in Mathematical Economics, 2006, Springer Japan.}.
       Let $\mathcal{L}^\Phi$ denote an Orlicz space%
	  \footnote{An Orlicz space is a function space which generalizes 
	            the $\mathcal{L}^p$ spaces. A Young function $\Phi:[0,\infty)\to[0,\infty)$ is a convex function 
	            with $\lim_{x\to\infty} \Phi(x)\to\infty$ and $\Phi(0)= 0$.
	            Given a Young function $\Phi$ and a probability space $\ofp$, define 
	            $L^\Phi\ofp= \{X:\Omega\to\Re, \text{ measurable}, \E[\Phi(|X|)] < \infty\}$
	            This set is not necessarily a vector space. The vector space spanned by $L^\Phi$
	            is the Orlicz space $\mathcal{L}^\Phi\ofp$. This space is equipped with the Luxemburg 
	            norm $\|X\|_\Phi = \inf\{\lambda>0:\E[\Phi(X/\lambda)]\leq 1\}$. We say that $\Phi$
	            has the $\Delta_2$ condition if $\Phi(2t) \leq K \Phi(t)$ for some $K>0$.}. 
       Any proper, (quasi)convex, law-invariant risk measure $\rho:\mathcal{L}^\Phi\to\barre$
       that is norm-lower semi-continuous has the Fatou property if and only if $\Phi$ is $\Delta_2$.
 
 \item (Kusuoka representations). Let $\ofp$ be a nonatomic space and let $\rho:\mathcal{L}^p\ofp\to\barre$ be a proper
       lower semi-continuous law-invariant coherent risk measure. Then, there exists a set $\mathfrak{M}$ of probability measures 
       on $[0,1)$ so that 
       \[
        \rho(Z) = \sup_{\mu\in\mathfrak{M}}\int_{0}^{1}\operatorname{AV@R}_{1-\alpha}(Z)\d\mu(\alpha),
       \]
       where $\operatorname{AV@R}_{1-\alpha}$ is the average value-at-risk operator at level $1-\alpha$ (defined 
       in the next section).

 \item (Regularity in spaces with atoms). Let $\ofp$ be a space with atoms and 
       $(\Omega, \HH, \prob)$ be a uniform probability space so that $\ofp$ is 
       isomorphic to it. 
       Let $\mathcal{Z}\dfn \mathcal{L}^p\ofp$ and $\hat{\mathcal{Z}} \dfn 
       \mathcal{L}^p(\Omega, \HH, \prob)$, $p\in[1,\infty)$. 
       Let $\hat{\rho}:\hat{\mathcal{Z}}\to\barre$ be a proper, lower semicontinuous,
       law invariant, coherent risk measure. We say that $\hat{\rho}$ is regular 
       if there is a proper, lower semicontinuous, law invariant, coherent 
       risk measure $\rho:\mathcal{Z}\to\barre$ so that $\rho_{|\hat{\mathcal{Z}}}=\hat{\rho}$.
       
 \item (Zero risk). Let $\ofp$ be a nonatomic probability space. Let $\rho$ be a proper, lower semicontinuous,
       coherent, law invariant risk measure. If $Z\in\mathcal{Z}$, $Z\geq 0$ a.s. then $\rho(Z)=0$ if and only if $Z=0$ a.s.
 \item (Risk under conditioning). Let $\ofp$ be a nonatomic space and $\rho:\mathcal{Z}\to\barre$ be a 
        proper convex lower semi-continuous  law-invariant risk measure. Let $\HH$ be a sub-$\sigma$-algebra 
        of $\F$. Then, $\rho(\ce{X})\leq \rho(X)$, for all $X\in\mathcal{Z}$ and $\E[X]\leq \rho(X)$.
  \item (Interchangeability principle for risk measures). Let $\mathcal{Z}\dfn \mathcal{L}^p\ofp$ and 
       $\mathcal{Z}' \dfn \mathcal{L}_{p'}\ofp$ with $p,p'\in[1,\infty]$. 
       Let $F:\Re^n\to\mathcal{Z}$, that is, for $x\in\Re^n$, $F(x)$ is a random variable; let
       $(F(x))(\omega)=f(x,\omega)$. For a set $X\subseteq \Re^n$ define 
       $\mathfrak{M}_X \dfn \{\chi \in \mathcal{Z}': \chi \in X,\, \prob\text{-a.s.}\}$.
       Let $\rho:\mathcal{Z}\to\barre$ be a proper monotone risk measure. 
       For $\chi\in\mathcal{Z}'$ define $F_\chi(\omega) = f(\chi(\omega), \omega)$
       Suppose that $\inf_{x\in X}F(x) \in \mathcal{Z}$
       and that $\rho$ is continuous at $\inf_{x\in X}F(x)$. Then
       \[
        \inf_{\chi \in \mathfrak{M}_X} \rho(F_\chi) = \rho\left(\inf_{x\in X}F(x)\right).
       \]
 \item (Interchangeability principle).        
\end{enumerate}


\subsection{Popular risk measures}
\begin{enumerate}
 \item (Trivially coherent risk measures). The expectation operator and the essential supremum are 
       coherent risk measures. For $\omega\in\Omega$, define $\rho(X) = X(\omega)$. This is a coherent
       risk measure, however, it is not law invariant.
       
 \item (Mean-Variance measure).  The mean-variance risk measure is defined as $\rho(X) = \E[X] + c \mathrm{Var}[X]$.
       This risk measure is law invariant, continuous, convex and translation equivariant. 
       However, it is neither monotone nor positively homogeneous.
       
 \item (Value-at-Risk). The Vale-at-Risk of a random variable $X$ at level $\alpha$ is the $(1-\alpha)$-quantile of $X$,
       that is, $\operatorname{V@R}_\alpha[X] = \inf\{t\in\Re: \prob[X>t]\leq \alpha\}$. $\operatorname{V@R}_\alpha$ 
       is monotone, positively homogeneous and translation equivariant, but nonconvex and not subadditive%
	  \footnote{The Value-at-Risk is convex for certain classes of random variables. 
	            See A. I. Kibzun and E. A. Kuznetsov, ``Convex Properties of the Quantile Function 
	            in Stochastic Programming,'' Automation and Remote Control, Vol. 65, No. 2, 2004, 
	            pp. 184--192.}.
	  
 \item (Average Value-at-Risk). The Average Value-at-Risk is defined as%
	  \footnote{We use the notation $[X]=\max\{X,0\}$. We use the definition of Shapiro et al. Other authors
	            use different definitions such as $\operatorname{AV@R}_\alpha[X] = \inf_{t\in\Re}t + \nicefrac{1}{1-\alpha}\E[X-t]_+$.}
       \[
          \operatorname{AV@R}_\alpha[X] = \inf_{t\in\Re}t + \nicefrac{1}{\alpha}\E[X-t]_+.
       \]
       This is a coherent law-invariant risk measure.
       
 \item (Mean-Deviation of order $p$). Let $X\in\mathcal{L}^p\ofp$, $p\in[1,\infty)$ and $c\geq 0$.
       Define
       \[
        \rho(X) = \E[X] + c \E\left[ |X-\E[X]|^p\right]^{\nicefrac{1}{p}}
       \]
       This is a convex, translation equivariant and positively homogeneous risk measure.
       It is monotone if $p=1$, $\ofp$ is nonatomic and $c\in[0,\nicefrac{1}{2}]$.
       
 \item (Mean-Upper-Semideviation of order $p$). Let $X\in\mathcal{L}^p\ofp$, $p\in[1,\infty)$ 
       and $c\geq 0$. The mapping
       \[
        \rho(X) = \E[X] + c \E\left[ [X-\E[X]]_+^p\right]^{\nicefrac{1}{p}}
       \]
       This is a convex, translation equivariant and positively homogeneous risk measure.
       It is monotone if $p=1$, $\ofp$ is nonatomic and $c\in[0,1]$.
       
 \item (Entropic risk measure). Let $\mathcal{Z}=\mathcal{L}^p\ofp$, $p\in[1,\infty]$.
       For $\gamma>0$, define the entropic risk measure
       \[
        \rho^{\mathrm{ent}}_\gamma(X)= \nicefrac{1}{\gamma}\E[e^{\gamma X}].
       \]
       For $p=\infty$, $\rho^{\mathrm{ent}}_\gamma$ is finite valued and w*-lower-semi-continuous.
       Moreover, $\rho^{\mathrm{ent}}_\gamma$ is convex, monotone and translation
       equivariant, but not positively homogeneous.
       Furthermore, $\lim_{\gamma \to 0}\rho^{\mathrm{ent}}_\gamma(X) = \E[X]$ and 
       $\lim_{\gamma \to \infty}\rho^{\mathrm{ent}}_\gamma(X) = \esssup[X]$.
       
 \item (Entropic Value-at-Risk). The entropic value-at-risk at level $1-\alpha$, $\alpha\in(0,1]$ of a random 
       variable $X$ for which the moment generating function $M_X$ exists is defined as%
	  \footnote{The moment generating function (MGF) $M_X$ of a random variable $X$ is defined 
	            as $M_X(z) \dfn \E[e^{zX}]$ for $z\in\Re$. Not all random variables have an MGF 
	            (e.g., the Cachy distribution does not define an MGF).%	            
	            }
       \[
        \operatorname{EV@R}_{1-\alpha}[X] = \inf_{t>0}\{\tfrac{1}{t}\ln(M_X(t)/\alpha\}.
       \]
       The entropic value-at-risk is a coherent risk measure for all $\alpha\in(0,1]$.
       
 \item (Expectiles). Let $X\in\mathcal{L}^2\ofp$ and $\tau\in(0,1)$. The $\tau$-expectile of 
       $X$ is defined as 
       \[
        e_{\tau}(X) = \argmin_{t\in\Re} \E[\tau [X-t]_+^2 + (1-\tau)[t-X]_+^2].
       \]
      For all $\tau\in(0,1)$, $e_{\tau}$ is a coherent risk measure.
 \item (Generalizations of $\operatorname{AV@R}_\alpha$)%
	     \footnote{%
	       These risk measures were first introduced by Ben-Tal and Teboulle; see for example
               A. Ben-Tal, M. Teboulle, ``An old–new concept of convex risk measures: an
	       optimized certainty equivalent,'' Mathematical Finance 17 (2007) 449--476.
	       These measures are discussed in: P. Krokhma, M. Zabarankin and S. Uryasev,
	       ``Modeling and optimization of risk,'' 
	       Surveys in Operations Research and Management Science 16 (2011) 49--66.%
	      }. 
	Let $X\in\mathcal{Z}\dfn \mathcal{L}^p\ofp$ and $\phi:\mathcal{Z}\to\Re_+$ be a function
	which is lower semicontinuous, monotone, convex and positive homogeneous. Then
	\[
	 \rho(X) = \inf_t \{t + \phi(X-t)\},
	\]
        is a coherent risk measure%
	      \footnote{In the case of $\operatorname{AV@R}_\alpha$, it is $\phi(X) = \nicefrac{1}{\alpha}\E[X]_+$
	      which is indeed convex, monotone and translation equivariant.}.
\end{enumerate}




\section{Bibliography with comments}
%
Bibliographic references including lecture notes and online resources with some comments:
{\small
\begin{enumerate}
 \item \label{cite:Gallager2013} \bibentry{Gallager2013}: A gentle introduction to stochastic 
        processes suitable for engineers who want to eschew the mathematical drudgery. 
        Following a short, but circumspect introduction to probability theory, the author 
        discusses several processes such as Poisson, Gaussian, Markovian and renewal processes. 
        Lastly, the book discusses hypothesis testing, martingales and estimation theory. 
        Without doubt, an excellent introduction to the topic for the uninitiated. 
 \item \label{cite:RLWolpert05} \bibentry{RLWolpert05}: Lecture notes with a succinct presentation 
	of some very useful results, but without many proofs. 
	Available at \url{https://www2.stat.duke.edu/courses/Spring05/sta205/lec/s05wk07.pdf}.
 \item \label{cite:cinlar2011} \bibentry{cinlar2011}: A fantastic book for one's first 
       steps in probability theory with 
       emphasis on random processes, filtrations, Martingales, stopping times and convergence 
       theorems, Poisson random measures, L{\'{e}}vy and Markovian processes and Brownian motion.
 \item \label{cite:Kallnberg} \bibentry{Kallnberg}: The definitive reference for researchers. In its 23 chapters
       it gives a circumspect overview of probability theory and stochastic processes; ideal for researchers in the field.
 \item \label{cite:KSigman2009}\bibentry{KSigman2009}: Lecture notes by K. Sigman, Columbia University,
       \url{http://www.columbia.edu/~ks20/stochastic-I/stochastic-I.html}.
 \item \label{cite:DWalnut2011} \bibentry{DWalnut2011}: A short compilation of convergence theorems       
 \item \label{cite:Varadhan} \bibentry{Varadhan}: A lot of material on limit theorems 
       starting from general measure theory, to weak convergence results, limits 
       of independent sums, results for dependent processes 
       with emphasis on Markov chains, a comprehensive introduction to martingales, stationary processes
       and ergodic theorems and some notes on dynamic programming. 
       Available online at \url{https://www.math.nyu.edu/faculty/varadhan/}.
 \item \label{cite:LinBai2011} \bibentry{LinBai2011}: several interesting (elementary 
       and advanced) inequalities on probability spaces.  
 \item \label{cite:Ambrosio2013} \bibentry{Ambrosio2013}: A short note at
       \url{http://planetmath.org/sites/default/files/texpdf/38346.pdf}
       showing that almost surely bounded RVs have all their moments bounded.
 \item \label{cite:SDR2014} \bibentry{SDR2014}: Excellent book on stochastic programming and 
       the definitive reference for risk measures.
\end{enumerate}
}


\bibliographystyle{plain}
\nobibliography{bibliography} 
\end{document}
